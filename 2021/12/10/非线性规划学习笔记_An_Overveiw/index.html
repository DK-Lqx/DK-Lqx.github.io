

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/browser_tab.jpeg">
  <link rel="icon" href="/img/browser_tab.jpeg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="学习非线性规划太激动了，遂总结笔记，激励自己不断进步">
  <meta name="author" content="lqx">
  <meta name="keywords" content="">
  <meta name="description" content="学习非线性规划太激动了，遂总结笔记，激励自己不断进步">
<meta property="og:type" content="article">
<meta property="og:title" content="非线性规划学习笔记：An Overview">
<meta property="og:url" content="http://example.com/2021/12/10/%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_An_Overveiw/index.html">
<meta property="og:site_name" content="Robotics and Astronomy by lqx">
<meta property="og:description" content="学习非线性规划太激动了，遂总结笔记，激励自己不断进步">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/NLP/好书1.jpg">
<meta property="og:image" content="http://example.com/img/NLP/好书2.jpeg">
<meta property="og:image" content="http://example.com/img/NLP/好书3.jpg">
<meta property="og:image" content="http://example.com/img/NLP/好书4.jpg">
<meta property="og:image" content="http://example.com/img/NLP/好书5.jpg">
<meta property="og:image" content="http://example.com/img/NLP/拉格朗日几何.jpg">
<meta property="og:image" content="http://example.com/img/NLP/KKT几何.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_in_NLP.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS.png">
<meta property="og:image" content="http://example.com/img/NLP/LS_Newton.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_Nerton_max.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_区间压缩.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_黄金分割法1.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_黄金分割法2.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_黄金分割法rou.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_斐波那契数列法rou.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_斐波那契数列2.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_斐波那契数列法1.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_Goldstein.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_Armijo.jpg">
<meta property="og:image" content="http://example.com/img/NLP/LS_Wolfe.jpg">
<meta property="og:image" content="http://example.com/img/NLP/无约束优化问题概览.jpg">
<meta property="og:image" content="http://example.com/img/NLP/最速下降法搜索方向正交.jpg">
<meta property="og:image" content="http://example.com/img/NLP/最速下降法收敛率1.jpg">
<meta property="og:image" content="http://example.com/img/NLP/最速下降法收敛率2.jpg">
<meta property="og:image" content="http://example.com/img/NLP/Newton_mthd.jpg">
<meta property="og:image" content="http://example.com/img/NLP/共轭梯度法_垂直于子空间.jpg">
<meta property="og:image" content="http://example.com/img/NLP/拟牛顿法模板.jpg">
<meta property="og:image" content="http://example.com/img/NLP/信赖域法框架1.jpg">
<meta property="og:image" content="http://example.com/img/NLP/信赖域法框架2.jpg">
<meta property="og:image" content="http://example.com/img/NLP/信赖域法算法.jpg">
<meta property="og:image" content="http://example.com/img/NLP/约束优化概览.jpg">
<meta property="article:published_time" content="2021-12-10T04:06:06.785Z">
<meta property="article:modified_time" content="2021-12-10T15:54:17.016Z">
<meta property="article:author" content="lqx">
<meta property="article:tag" content="Math">
<meta property="article:tag" content="Optimization">
<meta property="article:tag" content="Nonlinear Programming">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/NLP/好书1.jpg">
  
  <title>非线性规划学习笔记：An Overview - Robotics and Astronomy by lqx</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>DK-L</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/NLP/index.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="非线性规划学习笔记：An Overview">
              
                非线性规划学习笔记：An Overview
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-12-10 12:06" pubdate>
        December 10, 2021 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      41 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">非线性规划学习笔记：An Overview</h1>
            
            <div class="markdown-body">
              <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>最近快到期末了，认真的看了一下翟桥柱老师的运筹学课件，老师的课件做的详实且富有启发性。在阅读的时候不仅让我建立起了很多直观上的感受，并且跟随老师的步伐完成了大部分定理的证明。完成证明的时候深深感受到了数学这门学科的精妙，线性代数和多元函数微积分理论的优美，以及自己数学功底依然很薄弱的这个事实。遗憾的是，运筹学这门课的学时实在过于短暂，导致翟老师没有机会给我们讲解更多优美的算法和理论。为了对最优化理论有更加全面的理解，建立对于Optimization这门课程的整体认知，我又去看了《最优化导论》这本教材，其中介绍了更多的算法，并包括了更多定理证明的细节。在看这本书的时候，一些精妙的观点常常让我有醍醐灌顶之感，大部分精妙的数学证明又让我拍案叫绝，在同线性代数和多元函数微积分作斗争的时候，我仿佛又回到了大一那段沉迷数学的快乐时光，现在想来，我已经很久没有享受过陶醉于数学之美的那种快感了。感谢Optimization这门学科，让我没有捷径可走，只能直面数学，感受数学之美。最近埋头苦学最优化理论，看了很多书和论文，便想着写这一系列的学习笔记，总结一些自己的感悟，记录一些数学。</p>
<p>为什么题目叫非线性规划呢，是因为非线性规划不仅是Optimization最优化理论的核心，也在机器人学里得到了大量的应用，比如说trajectory optimization问题中的direct collocation method就是将积分离散化并转化为一个非线性规划得到求解。而且相对于线性规划来说，非线性规划更加困难也更加富有挑战性，常常要根据具体问题的特点选择不同的算法，因此对于不同问题和算法的特点都要有详细的了解，并且这种详细了解最好是数学的而非直觉的(因为要发论文的话数学一定要好…..)。</p>
<p>补充一点，优化理论真的是太重要了，在我感兴趣的领域内，微观和宏观经济学大量使用了最优化理论来解决资源配置和政策优化问题；机器人学中轨迹规划和最优控制都用到了非线性规划和动态规划等工具；机器学习和神经网络的基础更是非线性优化理论。因此，在我大三的时候写下这篇最优化理论学习笔记，就像最优化理论阐述的一样，希望能作为一个好的初始点，作为一个好的迭代起点，在之后学习最优化理论的优化迭代中，还有更加高级的数学理论等着我去学习，还要更加深入的学习凸优化、数值优化、随机优化等理论，并在经济学和机器人学等学科中去应用，去实践。前路漫漫，加油吧！</p>
<p>在这放上几本我正在看或者以后一定会看的优化领域的好书镇楼，也算激励一下自己：</p>
<center class="half">
    <img src="/img/NLP/好书1.jpg" srcset="/img/loading.gif" lazyload width="80"/>
    <img src="/img/NLP/好书2.jpeg" srcset="/img/loading.gif" lazyload width="120"/>
    <img src="/img/NLP/好书3.jpg" srcset="/img/loading.gif" lazyload width="80"/>
    <img src="/img/NLP/好书4.jpg" srcset="/img/loading.gif" lazyload width="90"/>
    <img src="/img/NLP/好书5.jpg" srcset="/img/loading.gif" lazyload width="90"/>
</center>




<h2 id="2-问题描述"><a href="#2-问题描述" class="headerlink" title="2 问题描述"></a>2 问题描述</h2><p>一般的非线性规划问题可以描述为：$$\min \ \  f(\boldsymbol{x})
$$ $$ s.t. \ \ \ \ \ g_i(\boldsymbol{x})\le0\ , i=1,2,\cdot\cdot\cdot,p
$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ h_i(\boldsymbol{x})=0\ , i=1,2,\cdot\cdot\cdot,m
$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \boldsymbol{x}\in \mathbb{R}^n$$</p>
<p>其中，函数$f:\mathbb{R}^n\to\mathbb{R}$称为目标函数或价值函数，该优化问题的含义为寻找合适的<script type="math/tex">\boldsymbol{x}</script>，使函数$f(\boldsymbol{x})$达到最小。<script type="math/tex">\boldsymbol{x}</script>是一个<script type="math/tex">n</script>维向量，通常称为决策向量，其需要同时满足等式约束<script type="math/tex">h_i(\boldsymbol{x})=0\ , i=1,2,\cdot\cdot\cdot,m</script>和不等式约束<script type="math/tex">g_i(\boldsymbol{x})\le0\ , i=1,2,\cdot\cdot\cdot,p</script>。</p>
<p>约束可以写成向量形式：                           $$\boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0} \ , \ \boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}$$</p>
<p>其中：                                                   $$\boldsymbol{h}:\mathbb{R}^n\to\mathbb{R}^m \ , \ \boldsymbol{g}:\mathbb{R}^n\to\mathbb{R}^p$$</p>
<p>研究和学习的过程的总是从特殊到一般的过程，因此我们不会在一开始就直面这么复杂的问题，而是要循序渐进，从最简单的情形开始研究，逐渐改变目标函数和约束的形式，最终得到最一般问题的求解方法。其中，每一种问题都有独特的结构和特色，利用这些结构和特色就可以发展出针对相应问题的最优算法。</p>
<p>故事的起点，是无约束或者集合约束优化问题：$$\min  \ \ \ \ \ f(\boldsymbol{x})
$$ $$ s.t. \ \ \ \ \boldsymbol{x}\in \Omega$$</p>
<p>其中约束集合<script type="math/tex">\Omega</script>是<script type="math/tex">n</script>维实数空间<script type="math/tex">\mathbb{R}^n</script>的一个子集。约束<script type="math/tex">\boldsymbol{x}\in \Omega</script>称为集合约束，上面提到的一般形式的约束集合<script type="math/tex">\Omega=\{ \boldsymbol{x}:\boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0} \ , \ \boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}\}</script>称为函数约束。由于集合约束比较简单，而且有一些很好的性质，所以有大量的算法都是针对于集合约束形式的，在一开始我们会先研究集合约束和无约束优化问题，这是一个很好的起点，后面更加复杂问题的算法很多也是基于无约束优化问题的算法进行改进。我们会先探讨局部极小点对应的必要条件和充分条件，以及在迭代过程中很重要的一维搜索算法，并讨论梯度方法、牛顿法、共轭方向法、拟牛顿法等多种求解算法，这些算法各有特色，使用时应根据问题的特点进行选择。</p>
<p>之后我们会研究有约束优化问题：$$\min \ \  f(\boldsymbol{x})
$$ $$ s.t. \ \ \ \ \ g_i(\boldsymbol{x})\le0\ , i=1,2,\cdot\cdot\cdot,p
$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ h_i(\boldsymbol{x})=0\ , i=1,2,\cdot\cdot\cdot,m
$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \boldsymbol{x}\in \mathbb{R}^n$$</p>
<p>本着循序渐进的原则，我们会先研究仅含等式约束的优化问题，展示局部极小点的一阶必要条件(拉格朗日条件)，以及二阶必要条件和充分条件。之后来到同时含义等式约束和不等式约束的优化问题，展示局部极小点的一阶必要条件(KKT条件)，以及二阶必要条件和充分条件。并研究相关的求解算法：投影法、拉格朗日法、罚函数法等。</p>
<h2 id="3-核心问题"><a href="#3-核心问题" class="headerlink" title="3 核心问题"></a>3 核心问题</h2><h3 id="3-1-算法框架"><a href="#3-1-算法框架" class="headerlink" title="3.1 算法框架"></a>3.1 算法框架</h3><p>在求解非线性规划的问题中，我们没有办法直接根据局部极小点的必要条件进行求解，因为那样就相当于直接求解大规模的非线性方程组，我们必须将问题转化为有限次运算可以解决的问题。所以说，我们必须放弃求解析解的想法，转而求解数值最优解。</p>
<p>一种朴实的想法如下：</p>
<p>我们先选择<script type="math/tex">\mathbb{R}^n</script>中的一个点作为起始点，然后依据某种迭代准则选取一个可以使目标函数下降的方向，沿着这个方向走一定的距离，到达一个新的位置，接着重复上述过程，选择一个下降方向，朝着这个方向走一定的距离以使目标函数下降，如此往复，直到满足停止准则（如局部极小点的必要条件得到满足），停止迭代，即认为得到了最优点。</p>
<p>用数学公式表述如下：$$\boldsymbol{x}^{(0)}\to \boldsymbol{x}^{(1)}\to \boldsymbol{x}^{(2)}\to \cdot\cdot\cdot\cdot\cdot\to\boldsymbol{x}^{(K)}$$</p>
<p>在从一个点迭代到下一个点的过程中，基本上所有非线性优化算法都采用相同的迭代框架：$$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)}$$</p>
<p>从上式中可以看出，在学习算法的过程中我们比较关心的问题有两个：</p>
<ul>
<li>搜索方向<script type="math/tex">\boldsymbol{p}^{(k)}</script>的确定：为不同算法讨论的核心问题</li>
<li>搜索步长<script type="math/tex">t_k</script>的确定：采用一维搜索算法得到解决</li>
</ul>
<p>求解非线性规划的迭代下降算法框架为：</p>
<ul>
<li><p>Step1：   产生初始点<script type="math/tex">\boldsymbol{x}^{(0)}\in\Omega</script>或<script type="math/tex">\boldsymbol{x}^{(0)}\in\mathbb{R}^n</script>，初始化<script type="math/tex">k=0</script></p>
</li>
<li><p>Step2：   由<script type="math/tex">\boldsymbol{x}^{(k)}</script>产生<script type="math/tex">\boldsymbol{x}^{(k+1)}</script>，并令<script type="math/tex">k:=k+1</script></p>
<p>​                  一般<script type="math/tex">\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)}</script></p>
<p>​                  核心为构造<script type="math/tex">\boldsymbol{p}^{(k)}</script>并选择合适的<script type="math/tex">t_k</script></p>
</li>
<li><p>Step3：判断<script type="math/tex">\boldsymbol{x}^{(k)}</script>是否可接受，是则停止迭代，否则转Step2</p>
</li>
</ul>
<h3 id="3-2-关键问题"><a href="#3-2-关键问题" class="headerlink" title="3.2 关键问题"></a>3.2 关键问题</h3><p>翟老师在课上总结了非线性规划的六个关键问题，总结的简直太好了，只有把一个算法的这六个问题搞懂了，才算是真正弄懂了这个算法。故摘录如下，在接下来的文章中也是按照这个顺序一一总结各个算法的相应特色。</p>
<p>在研究和学习非线性规划时面临的六个关键问题如下：</p>
<ul>
<li>(局部)最优解有哪些性质？如何判定？</li>
<li>什么样的全局性质可以使得寻找全局最优解比较容易？</li>
<li>怎样构造(可行的)下降方向<script type="math/tex">\boldsymbol{p}^{(k)}</script>？</li>
<li>怎样确定步长<script type="math/tex">t_k</script>？</li>
<li>怎样确定初始点<script type="math/tex">\boldsymbol{x}^{(0)}</script>？</li>
<li>算法的终止准则？</li>
</ul>
<p>下面会从这六个方面入手总结各个算法的特点。</p>
<h2 id="4-局部最优解的性质"><a href="#4-局部最优解的性质" class="headerlink" title="4 局部最优解的性质"></a><strong>4 局部最优解的性质</strong></h2><h3 id="4-1-无约束优化"><a href="#4-1-无约束优化" class="headerlink" title="4.1 无约束优化"></a>4.1 无约束优化</h3><p>回顾一下，无约束优化和集合约束的问题为：$$\min  \ \ \ \ \ f(\boldsymbol{x})
$$ $$ s.t. \ \ \ \ \boldsymbol{x}\in \Omega$$</p>
<h4 id="4-1-1-一阶必要条件"><a href="#4-1-1-一阶必要条件" class="headerlink" title="4.1.1 一阶必要条件"></a>4.1.1 一阶必要条件</h4><p>假设局部极小点位于在约束集<script type="math/tex">\Omega</script>的内部，则极小点满足的一阶必要条件为：$$\nabla f(\boldsymbol{x}^*)=\boldsymbol{0}$$</p>
<h4 id="4-1-2-二阶必要条件"><a href="#4-1-2-二阶必要条件" class="headerlink" title="4.1.2 二阶必要条件"></a>4.1.2 二阶必要条件</h4><p>假设局部极小点位于在约束集<script type="math/tex">\Omega</script>的内部，则极小点满足的二阶必要条件为：$$\nabla f(\boldsymbol{x}^*)=\boldsymbol{0}$$   $$\nabla^2 f(\boldsymbol{x}^*)\ge0$$</p>
<p>即<script type="math/tex">f(\boldsymbol{x})</script>的Hessian矩阵<script type="math/tex">\nabla^2 f(\boldsymbol{x})</script>在<script type="math/tex">\boldsymbol{x}^*</script>处是半正定的。</p>
<h4 id="4-1-3-二阶充分条件"><a href="#4-1-3-二阶充分条件" class="headerlink" title="4.1.3 二阶充分条件"></a>4.1.3 二阶充分条件</h4><p>若<script type="math/tex">\boldsymbol{x}^*</script>是约束集的一个内点，且同时满足：$$\nabla f(\boldsymbol{x}^*)=\boldsymbol{0}$$ $$\nabla^2 f(\boldsymbol{x}^*)>0$$  </p>
<p>即<script type="math/tex">\boldsymbol{x}^*</script>处<script type="math/tex">f(\boldsymbol{x})</script>的梯度为0且Hessian矩阵为正定，则<script type="math/tex">\boldsymbol{x}^*</script>是函数<script type="math/tex">f</script>的一个严格局部极小点。                                         </p>
<h3 id="4-2-等式约束"><a href="#4-2-等式约束" class="headerlink" title="4.2 等式约束"></a>4.2 等式约束</h3><p>本节将研究以下形式约束问题局部极小点对应的条件：</p>
<p>​                                                                         $$\min  \ \ \ \ \ \ \ \ f(\boldsymbol{x})
$${% raw %}{% endraw %}<script type="math/tex">s.t. \ \ \ \ \boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}</script></p>
<p>在讨论含有等式约束的优化问题时，有一个很重要的概念叫做正则点，在正则点处，所有等式约束的梯度向量线性无关，即<script type="math/tex">\nabla h_1(\boldsymbol{x}),\nabla h_2(\boldsymbol{x}),...,\nabla h_m(\boldsymbol{x})</script>线性无关。</p>
<p>接下来还要用到切线空间和法线空间的概念，定义如下：</p>
<p>在曲面<script type="math/tex">S=\{\boldsymbol{x}\in \mathbb{R}^n,\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}\}</script>中：</p>
<ul>
<li><p>点<script type="math/tex">\boldsymbol{x}^*</script>处的切线空间为集合<script type="math/tex">T(\boldsymbol{x}^*)=\{\boldsymbol{y}:D\boldsymbol{h}(\boldsymbol{x}^*)\boldsymbol{y}=\boldsymbol{0}\}</script>，为矩阵<script type="math/tex">D\boldsymbol{h}(\boldsymbol{x}^*)</script>的零空间。</p>
</li>
<li><p>点<script type="math/tex">\boldsymbol{x}^*</script>处的法线空间为集合<script type="math/tex">N(\boldsymbol{x}^*)=\{\boldsymbol{x}\in \mathbb{R}^n:\boldsymbol{x}=D\boldsymbol{h}(\boldsymbol{x}^*)^T\boldsymbol{z},\boldsymbol{z}\in \mathbb{R}^m\}</script>，为矩阵<script type="math/tex">D\boldsymbol{h}(\boldsymbol{x}^*)^T</script>的值域。</p>
</li>
</ul>
<p>可以证明，切线空间和法线空间互为正交补。</p>
<h4 id="4-2-1-一阶必要条件-拉格朗日定理"><a href="#4-2-1-一阶必要条件-拉格朗日定理" class="headerlink" title="4.2.1 一阶必要条件(拉格朗日定理)"></a>4.2.1 一阶必要条件(拉格朗日定理)</h4><p>拉格朗日定理可表述如下：</p>
<p>若<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f:\mathbb{R}^n\to\mathbb{R}</script>的局部极小点(或极大点)，约束条件为<script type="math/tex">\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}</script>，<script type="math/tex">\boldsymbol{h}:\mathbb{R}^n\to\mathbb{R}^m,m\le n</script>。如果<script type="math/tex">\boldsymbol{x}^*</script>是正则点，那么存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>，使得：$$Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)=\boldsymbol{0}^T$$</p>
<p>其中：</p>
<ul>
<li>第一项中<script type="math/tex">Df(\boldsymbol{x})</script>为函数<script type="math/tex">f</script>的一阶导数：<script type="math/tex">Df=[\frac{\partial{f}}{\partial{x_1}},\frac{\partial{f}}{\partial{x_2}},...,\frac{\partial{f}}{\partial{x_n}}]</script>，刚好为梯度<script type="math/tex">\nabla f(\boldsymbol{x})</script>的转置。</li>
<li>第二项中<script type="math/tex">D\boldsymbol{h}(\boldsymbol{x})</script>为向量值函数<script type="math/tex">\boldsymbol{h}(\boldsymbol{x})</script>的雅克比矩阵：$$D\boldsymbol{h}(\boldsymbol{x})=\begin{bmatrix}Dh_1(\boldsymbol{x})\\ ...\\Dh_m(\boldsymbol{x})\end{bmatrix}=\begin{bmatrix}\nabla h_1(\boldsymbol{x})^T\\ ...\\\nabla h_m(\boldsymbol{x})^T\end{bmatrix}$$</li>
</ul>
<p>将上式展开，就会发现：$$\nabla f(\boldsymbol{x}^*)^T = -\begin{bmatrix} \lambda_1 & ... &\lambda_m \end{bmatrix} \begin{bmatrix}\nabla h_1(\boldsymbol{x}^*)^T\\ ...\\\nabla h_m(\boldsymbol{x}^*)^T\end{bmatrix}$$</p>
<p>拉格朗日定理表明，如果<script type="math/tex">\boldsymbol{x}^*</script>是极值点，那么目标函数<script type="math/tex">f</script>在该点处梯度可以表示为关于约束函数在该点梯度的线性组合。</p>
<p>证明是构造性的，证明<script type="math/tex">\nabla f(\boldsymbol{x}^*)</script>位于法线空间中即可。</p>
<p>拉格朗日定理的几何解释如下图所示：</p>
<p><img src="/img/NLP/拉格朗日几何.jpg" srcset="/img/loading.gif" lazyload alt="拉格朗日几何" style="zoom:25%;" /></p>
<p>为了便于描述，引入拉格朗日函数：<script type="math/tex">l(\boldsymbol{x},\boldsymbol{\lambda})=f(\boldsymbol{x})+\boldsymbol{\lambda}^T\boldsymbol{h}(\boldsymbol{x})</script></p>
<p>则一阶必要条件可以重新表述为：$$D_xl(\boldsymbol{x}^*,\boldsymbol{\lambda}^*)=\boldsymbol{0}^T$$ $$D_{\lambda}l(\boldsymbol{x}^*,\boldsymbol{\lambda}^*)=\boldsymbol{0}^T$$</p>
<h4 id="4-2-2-二阶必要条件"><a href="#4-2-2-二阶必要条件" class="headerlink" title="4.2.2 二阶必要条件"></a>4.2.2 二阶必要条件</h4><p>记<script type="math/tex">\boldsymbol{L}(\boldsymbol{x},\boldsymbol{\lambda})</script>是<script type="math/tex">l(\boldsymbol{x},\boldsymbol{\lambda})</script>关于<script type="math/tex">\boldsymbol{x}</script>的Hessian矩阵：$$\boldsymbol{L}(\boldsymbol{x},\boldsymbol{\lambda})=\nabla^2 f(\boldsymbol{x})+\lambda_1\nabla^2 h_1(\boldsymbol{x})+...+\lambda_m\nabla^2 h_m(\boldsymbol{x})$$</p>
<p>若<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f:\mathbb{R}^n\to\mathbb{R}</script>的局部极小点，且<script type="math/tex">\boldsymbol{x}^*</script>为正则点，则存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>，使得：</p>
<p>1、                                                        <script type="math/tex">Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)=\boldsymbol{0}^T</script></p>
<p>2、                                      对于所有<script type="math/tex">\boldsymbol{y}\in T(\boldsymbol{x}^*)</script>，都有<script type="math/tex">\boldsymbol{y}^T\boldsymbol{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*)\boldsymbol{y}\ge0</script>                         </p>
<h4 id="4-2-3-二阶充分条件"><a href="#4-2-3-二阶充分条件" class="headerlink" title="4.2.3 二阶充分条件"></a>4.2.3 二阶充分条件</h4><p>如果存在<script type="math/tex">\boldsymbol{x}^*\in\mathbb{R}^n</script>和<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>，使得：</p>
<p>1、                                                     <script type="math/tex">Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)=\boldsymbol{0}^T</script></p>
<p>2、                                      对于所有<script type="math/tex">\boldsymbol{y}\in T(\boldsymbol{x}^*)</script>，<script type="math/tex">\boldsymbol{y}\ne\boldsymbol{0}</script>，都有<script type="math/tex">\boldsymbol{y}^T\boldsymbol{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*)\boldsymbol{y}>0</script>     </p>
<p>那么<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f</script>在约束条件<script type="math/tex">\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}</script>下的严格局部极小点。    </p>
<h3 id="4-3-等式和不等式约束"><a href="#4-3-等式和不等式约束" class="headerlink" title="4.3 等式和不等式约束"></a>4.3 等式和不等式约束</h3><p>本节将研究以下形式约束问题局部极小点对应的条件：$$\min  \ \ \ \ \ \ \ \ f(\boldsymbol{x})
$$   $$ s.t. \ \ \ \ \boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}$$  $$ \ \ \ \ \ \ \ \ \ \ \ \boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0}$$</p>
<p>在讨论含不等式约束的优化问题时，一个很重要的概念是起作用约束和不起作用约束。若对于一个不等式约束<script type="math/tex">g_j(\boldsymbol{x})\le0</script>，如果在<script type="math/tex">\boldsymbol{x}^*</script>处<script type="math/tex">g_j(\boldsymbol{x^*})=0</script>，那么称该约束是<script type="math/tex">\boldsymbol{x}^*</script>处的起作用约束；如果在<script type="math/tex">\boldsymbol{x}^*</script>处<script type="math/tex">g_j(\boldsymbol{x^*})<0</script>，那么称该约束是<script type="math/tex">\boldsymbol{x}^*</script>处的不起作用约束。</p>
<p>为了区分起作用约束和不起作用约束，用<script type="math/tex">J(\boldsymbol{x^*})</script>表示起作用不等式约束的下标集：$$J(\boldsymbol{x^*})=\{j:g_j(\boldsymbol{x^*})=0\}$$</p>
<p>同样的，在含不等式约束优化问题中，正则点的概念同样重要。在<script type="math/tex">\boldsymbol{x^*}</script>满足约束的前提下，如果向量集<script type="math/tex">\nabla h_i(\boldsymbol{x^*}),\nabla g_j(\boldsymbol{x^*}),1\le i\le m,j\in J(\boldsymbol{x^*})</script>是线性无关的，那么则称<script type="math/tex">\boldsymbol{x^*}</script>是一个正则点。</p>
<h4 id="4-3-1-一阶必要条件-KKT条件"><a href="#4-3-1-一阶必要条件-KKT条件" class="headerlink" title="4.3.1 一阶必要条件(KKT条件)"></a>4.3.1 一阶必要条件(KKT条件)</h4><p>若<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f:\mathbb{R}^n\to\mathbb{R}</script>的局部极小点，约束条件为<script type="math/tex">\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0},\boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0}</script>。如果<script type="math/tex">\boldsymbol{x}^*</script>是正则点，那么存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>和<script type="math/tex">\boldsymbol{\mu}^*\in\mathbb{R}^p</script>，使得：</p>
<p>1、                                                                            $$\boldsymbol{\mu}^*\ge0$$</p>
<p>2、                                               $$Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)+{\boldsymbol{\mu}^*}^TD\boldsymbol{g}(\boldsymbol{x}^*)=\boldsymbol{0}^T$$</p>
<p>3、                                                                      $${\boldsymbol{\mu}^*}^T\boldsymbol{g}(\boldsymbol{x}^*)=0$$</p>
<p>上面三个条件被称为KKT条件（Karush-Kuhn-Tucker条件）。</p>
<p>KKT条件的几何解释如下图所示：</p>
<p><img src="/img/NLP/KKT几何.jpg" srcset="/img/loading.gif" lazyload alt="KKT几何" style="zoom:25%;" /></p>
<h4 id="4-3-2-二阶必要条件"><a href="#4-3-2-二阶必要条件" class="headerlink" title="4.3.2 二阶必要条件"></a>4.3.2 二阶必要条件</h4><p>类似的，定义<script type="math/tex">\boldsymbol{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\mu})</script>： $$\boldsymbol{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\mu})=\nabla^2 f(\boldsymbol{x})+\lambda_1\nabla^2 h_1(\boldsymbol{x})+...+\lambda_m\nabla^2 h_m(\boldsymbol{x})+\mu_1\nabla^2 g_1(\boldsymbol{x})+...+\mu_p\nabla^2 g_p(\boldsymbol{x})$$</p>
<p>并用<script type="math/tex">T(\boldsymbol{x}^*)=\{\boldsymbol{y}\in\mathbb{R}^n:D\boldsymbol{h}(\boldsymbol{x}^*)\boldsymbol{y}=\boldsymbol{0},Dg_j(\boldsymbol{x}^*)\boldsymbol{y}=\boldsymbol{0},j\in J(\boldsymbol{x}^*)\}</script>代表起作用约束所定义曲面的切线空间。</p>
<p>则二阶必要条件为：</p>
<p>若<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f:\mathbb{R}^n\to\mathbb{R}</script>的局部极小点，且<script type="math/tex">\boldsymbol{x}^*</script>为正则点，则存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m，\boldsymbol{\mu}^*\in\mathbb{R}^p</script>，使得：</p>
<p>1、                             <script type="math/tex">\boldsymbol{\mu}^*\ge0,Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)+{\boldsymbol{\mu}^*}^TD\boldsymbol{g}(\boldsymbol{x}^*)=\boldsymbol{0}^T,{\boldsymbol{\mu}^*}^T\boldsymbol{g}(\boldsymbol{x}^*)=0</script></p>
<p>2、                                       对于所有<script type="math/tex">\boldsymbol{y}\in T(\boldsymbol{x}^*)</script>，都有<script type="math/tex">\boldsymbol{y}^T\boldsymbol{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*,\boldsymbol{\mu}^*)\boldsymbol{y}\ge0</script>              </p>
<h4 id="4-3-3-二阶充分条件"><a href="#4-3-3-二阶充分条件" class="headerlink" title="4.3.3 二阶充分条件"></a>4.3.3 二阶充分条件</h4><p>如果<script type="math/tex">\boldsymbol{x}^*\in\mathbb{R}^n</script>是一个可行点，存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>和<script type="math/tex">\boldsymbol{\mu}^*\in\mathbb{R}^p</script>，使得：</p>
<p>1、                             <script type="math/tex">\boldsymbol{\mu}^*\ge0,Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)+{\boldsymbol{\mu}^*}^TD\boldsymbol{g}(\boldsymbol{x}^*)=\boldsymbol{0}^T,{\boldsymbol{\mu}^*}^T\boldsymbol{g}(\boldsymbol{x}^*)=0</script></p>
<p>2、                                       对于所有<script type="math/tex">\boldsymbol{y}\in \widetilde{T}(\boldsymbol{x}^*)</script>，<script type="math/tex">\boldsymbol{y}\ne\boldsymbol{0}</script>，都有<script type="math/tex">\boldsymbol{y}^T\boldsymbol{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*,\boldsymbol{\mu}^*)\boldsymbol{y}>0</script>             </p>
<p>那么<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f</script>在约束条件<script type="math/tex">\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0},\boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0}</script>下的严格局部极小点。   </p>
<p>其中，<script type="math/tex">\widetilde{T}(\boldsymbol{x}^*)=\{\boldsymbol{y}\in\mathbb{R}^n:D\boldsymbol{h}(\boldsymbol{x}^*)\boldsymbol{y}=\boldsymbol{0},Dg_j(\boldsymbol{x}^*)\boldsymbol{y}=\boldsymbol{0},j\in \widetilde{J}(\boldsymbol{x}^*,\boldsymbol{\mu}^*)\}</script></p>
<p>上式中，<script type="math/tex">\widetilde{J}(\boldsymbol{x}^*,\boldsymbol{\mu}^*)=\{i:g_i(\boldsymbol{x}^*=0,\mu_i^*>0\}</script></p>
<h2 id="5-全局性质：凸性Convexity"><a href="#5-全局性质：凸性Convexity" class="headerlink" title="5 全局性质：凸性Convexity"></a><strong>5 全局性质：凸性Convexity</strong></h2><h3 id="5-1-为什么研究凸优化"><a href="#5-1-为什么研究凸优化" class="headerlink" title="5.1 为什么研究凸优化"></a>5.1 为什么研究凸优化</h3><p>在实际中，我们经常会碰到目标函数是凸函数、约束集是凸集的优化问题，这类问题称为凸优化问题或者凸规划。线性规划、二次规划（目标函数为二次型函数、约束方程为线性方程）等实际中大量出现的问题都属于凸优化的范畴。而且凸优化问题有两个很好的性质：</p>
<ul>
<li>局部极小点就是全局极小点</li>
<li>极小点的一阶必要条件同时是凸优化问题的充分条件</li>
</ul>
<p>这些性质为求解凸优化问题带来了很大的方便，因此很有必要研究一下凸优化问题。</p>
<p>下面先来讨论几个很能体现凸优化问题特色的定理：</p>
<ul>
<li><p>定理一：已知<script type="math/tex">f:\Omega\to\mathbb{R}</script>是定义在凸集<script type="math/tex">\Omega\subset\mathbb{R}^n</script>上的凸函数，集合<script type="math/tex">\Omega</script>中某一点是<script type="math/tex">f</script>的全局极小点，当且仅当它是<script type="math/tex">f</script>的局部极小点。</p>
</li>
<li><p>定理二：如果<script type="math/tex">f:\Omega\to\mathbb{R}</script>是定义在凸集<script type="math/tex">\Omega\subset\mathbb{R}^n</script>上的凸函数，若存在<script type="math/tex">\boldsymbol{x}^*\in\Omega</script>，使得：$$\nabla f(\boldsymbol{x}^*)=\boldsymbol{0}$$</p>
<p>则<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f</script>在<script type="math/tex">\Omega</script>上的全局极小点。这说明集合约束对应的凸优化问题中一阶必要条件就是全局极小的充分条件。</p>
</li>
<li><p>定理三：如果<script type="math/tex">f:\Omega\to\mathbb{R}</script>是定义在凸集<script type="math/tex">\Omega=\{\boldsymbol{x}\in \mathbb{R}^n,\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0}\}</script>上的凸函数，且<script type="math/tex">\Omega</script>是凸集。假设存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>，使得：$$Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)=\boldsymbol{0}^T$$</p>
<p>则<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f</script>在<script type="math/tex">\Omega</script>上的全局极小点。这说明等式约束对应的凸优化问题中拉格朗日条件就是全局极小的充分条件。</p>
</li>
<li><p>定理四：如果<script type="math/tex">f:\Omega\to\mathbb{R}</script>是定义在凸集<script type="math/tex">\Omega=\{\boldsymbol{x}\in \mathbb{R}^n,\boldsymbol{h}(\boldsymbol{x})=\boldsymbol{0},\boldsymbol{g}(\boldsymbol{x})\le\boldsymbol{0}\}</script>上的凸函数，且<script type="math/tex">\Omega</script>是凸集。假设存在<script type="math/tex">\boldsymbol{\lambda}^*\in\mathbb{R}^m</script>和<script type="math/tex">\boldsymbol{\mu}^*\in\mathbb{R}^p</script>，使得：$$\boldsymbol{\mu}^*\ge0$$ $$Df(\boldsymbol{x}^*)+{\boldsymbol{\lambda}^*}^TD\boldsymbol{h}(\boldsymbol{x}^*)+{\boldsymbol{\mu}^*}^TD\boldsymbol{g}(\boldsymbol{x}^*)=\boldsymbol{0}^T$$ $${\boldsymbol{\mu}^*}^T\boldsymbol{g}(\boldsymbol{x}^*)=0$$</p>
<p>则<script type="math/tex">\boldsymbol{x}^*</script>是<script type="math/tex">f</script>在<script type="math/tex">\Omega</script>上的全局极小点。这说明不等式约束对应的凸优化问题中KKT条件就是全局极小的充分条件。</p>
</li>
</ul>
<h3 id="5-2-基本定义与性质"><a href="#5-2-基本定义与性质" class="headerlink" title="5.2 基本定义与性质"></a>5.2 基本定义与性质</h3><p>几个基本的定义如下：</p>
<ul>
<li><p>凸集：若对于所有的<script type="math/tex">\boldsymbol{u},\boldsymbol{v}\in\Theta</script>，<script type="math/tex">\boldsymbol{u}</script>和<script type="math/tex">\boldsymbol{v}</script>之间的线段<script type="math/tex">\alpha\boldsymbol{u}+(1-\alpha)\boldsymbol{v},\alpha\in[0,1]</script>都位于<script type="math/tex">\Theta</script>内，称<script type="math/tex">\Theta\in\mathbb{R}^n</script>为凸集。</p>
</li>
<li><p>凸函数：对于定义在凸集<script type="math/tex">\Omega\in\mathbb{R}^n</script>上的函数<script type="math/tex">f:\Omega\to\mathbb{R}</script>，则函数<script type="math/tex">f</script>是凸函数当且仅当对于任意<script type="math/tex">\boldsymbol{x},\boldsymbol{y}\in\Omega,\boldsymbol{x}\ne\boldsymbol{y}</script>和<script type="math/tex">\alpha\in(0,1)</script>，都有： $$f(\alpha\boldsymbol{x}+(1-\alpha)\boldsymbol{y})\le \alpha f(\boldsymbol{x})+(1-\alpha)f(\boldsymbol{y})$$</p>
<p>​               </p>
</li>
</ul>
<p>凸函数比较重要的性质总结如下：</p>
<ul>
<li>若干凸函数的线性组合还是凸函数。</li>
<li>凸函数的定义域必为凸集。</li>
<li>函数<script type="math/tex">f</script>是凸函数当且仅当对于任意<script type="math/tex">\boldsymbol{x},\boldsymbol{y}\in\Omega</script>，有<script type="math/tex">f(\boldsymbol{y})\ge f(\boldsymbol{x})+Df(\boldsymbol{x})(\boldsymbol{y}-\boldsymbol{x})</script>。几何意义为凸函数的图像总是在其线性近似函数的上方。</li>
<li>二次型函数<script type="math/tex">f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x},Q=Q^T</script>是凸函数当且仅当矩阵<script type="math/tex">Q</script>半正定。</li>
<li>函数<script type="math/tex">f</script>是凸函数当且仅当<script type="math/tex">f</script>的Hessian矩阵半正定，即<script type="math/tex">\nabla^2 f(\boldsymbol{x}^*)\ge0</script>。</li>
</ul>
<h3 id="5-3-半定规划"><a href="#5-3-半定规划" class="headerlink" title="5.3 半定规划"></a>5.3 半定规划</h3><p>半定规划是凸优化的一个分支，求解的是线性矩阵不等式约束下的线性目标函数的极小值。线性矩阵不等式约束定义了一个凸集，要求在该可行集上使得目标函数达到极小值。</p>
<h2 id="6-确定步长：一维搜索方法"><a href="#6-确定步长：一维搜索方法" class="headerlink" title="6 确定步长：一维搜索方法"></a><strong>6 确定步长：一维搜索方法</strong></h2><h3 id="6-1-why-Line-Search？"><a href="#6-1-why-Line-Search？" class="headerlink" title="6.1 why Line Search？"></a>6.1 why Line Search？</h3><p>在上面我们提到，非线性规划中每一步迭代都使用如下的公式确定每一次迭代的方向和步长：$$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)}$$</p>
<p>其中<script type="math/tex">\boldsymbol{p}^{(k)}</script>为搜索方向，是下一大节主要讨论的问题，在这里我们认为每一次迭代的搜索方向<script type="math/tex">\boldsymbol{p}^{(k)}</script>是已知的。在每一次迭代的过程中，我们肯定都希望结果相较前一次有所改善，也就是希望每一次迭代都能使目标函数值有所下降，即<script type="math/tex">f(\boldsymbol{x}^{(k+1)})<f(\boldsymbol{x}^{(k)})</script>，甚至是下降的最多。而我们现在唯一可以调整的参量就是搜索步长<script type="math/tex">t_k</script>，在<script type="math/tex">\boldsymbol{p}^{(k)}</script>和<script type="math/tex">\boldsymbol{x}^{(k)}</script>都确定的情况下，<script type="math/tex">f(\boldsymbol{x}^{(k+1)})</script>只与<script type="math/tex">t_k</script>的取值有关，是关于<script type="math/tex">t_k</script>的一元函数。一个很直观的想法就是找出使得<script type="math/tex">f(\boldsymbol{x}^{(k+1)})</script>最小的<script type="math/tex">t_k</script>，即：$$t_k=\underset{t_k>0}{argmin}\ \ f(\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)})$$</p>
<p><img src="/img/NLP/LS_in_NLP.jpg" srcset="/img/loading.gif" lazyload alt="LS_in_NLP" style="zoom:25%;" /></p>
<p>如图所示，一维搜索方法的主要目的就是：</p>
<p>找到使得<script type="math/tex">f(\boldsymbol{x}^{(k+1)})=f(\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)})=\phi(t_k)</script>有所下降甚至是取极小值的<script type="math/tex">t_k</script>，进而确定每一次迭代步长。</p>
<h3 id="6-2-六点说明"><a href="#6-2-六点说明" class="headerlink" title="6.2 六点说明"></a>6.2 六点说明</h3><ol>
<li><p>适用范围：一维搜索问题可用于解决一元函数极小化和非线性规划中每一次迭代步长的确定。</p>
</li>
<li><p>一维搜索方法及本质：</p>
<ul>
<li>仍具有一般非线性规划问题的结构，因为需要极小化的函数<script type="math/tex">f(\boldsymbol{x}^{(k)}+t_k\boldsymbol{p}^{(k)})</script>大部分是非线性的</li>
<li>非线性规划的迭代下降算法本质是把<script type="math/tex">n</script>维问题化为一系列一维问题</li>
<li>一维搜索问题仍采用迭代(下降)算法</li>
<li>搜索方向可正可负，产生一系列试探点列：<script type="math/tex">t_0,t_1,...,t_K</script></li>
</ul>
</li>
<li><p>几个概念：</p>
<ul>
<li><p>全局最优解、局部最优解、满意解：字面意思</p>
</li>
<li><p>单谷函数：区间<script type="math/tex">[a,b]</script>的单谷函数<script type="math/tex">\phi(t)</script>：<script type="math/tex">\exist\ t^*\in[a,b]</script>，使<script type="math/tex">\phi(t)</script>在<script type="math/tex">[a,t^*]</script>严格递减，且在<script type="math/tex">[t^*,b]</script>严格递增。</p>
<p>​                   注意单谷函数和凸函数是两个概念。</p>
</li>
</ul>
</li>
<li><p>精确一维搜索与非精确一维搜索：</p>
<ul>
<li>精确一维搜索：试探点列<script type="math/tex">\{t_l\}_{l=0}^{+\infty}</script>满足<script type="math/tex">\underset{l\to +\infty}{\lim}t_l=t^*</script>，即可以精确的找到极小点对应的<script type="math/tex">t</script></li>
<li>非精确一维搜索：不能保证试探点列收敛到任意局部极小值，但可以保证目标函数值有一定的下降</li>
<li>实际应用中均仅执行有限步试探</li>
</ul>
</li>
<li><p>划界问题：如何确定初始搜索区间？即：确定<script type="math/tex">[a,b]</script>或<script type="math/tex">t_{max}</script>。有专门的划界算法解决该问题。</p>
</li>
<li><p>常用一维搜索算法总结：</p>
<p><img src="/img/NLP/LS.png" srcset="/img/loading.gif" lazyload alt="LS" style="zoom:30%;" /></p>
</li>
<li><p>总体评价：</p>
<ul>
<li>当用于求解非线性规划中定步长子问题时，非精确一维搜索效果要明显好于精确一维搜索。</li>
<li>各种非精确一维搜索方法之间差别不大。</li>
</ul>
</li>
</ol>
<h3 id="6-3-精确一维搜索"><a href="#6-3-精确一维搜索" class="headerlink" title="6.3 精确一维搜索"></a>6.3 精确一维搜索</h3><p>精确一维搜索致力于寻找目标函数的局部或全局最优解，主要有两种思路：</p>
<ul>
<li>区间压缩算法：通过不断缩小包含极小点的区间长度，进而找到一个极小点最够好的近似。主要包括黄金分割法、斐波那契数列法、二分法等。</li>
<li>通过求解近似函数的极小点迭代求解：主要代表有牛顿法，每次迭代都将原函数的二次近似函数的极小点作为下一个迭代点，使目标函数值不断下降，最终找到极小点。</li>
</ul>
<p>下面依次介绍这四种算法。</p>
<h4 id="6-3-1-牛顿法"><a href="#6-3-1-牛顿法" class="headerlink" title="6.3.1 牛顿法"></a>6.3.1 牛顿法</h4><p>牛顿法的思想很简单，就是在当前迭代点处使用一个二次函数去近似目标函数，然后使用这个二次函数的极小点作为下一个迭代点，不断去逼近原函数的极小点。</p>
<p><img src="/img/NLP/LS_Newton.jpg" srcset="/img/loading.gif" lazyload alt="LS_Newton" style="zoom:25%;" /></p>
<p>如上图所示，我们在迭代点<script type="math/tex">x^{(k)}</script>处用二次函数<script type="math/tex">q(x)</script>近似原来的目标函数<script type="math/tex">f(x)</script>，进而将二次函数<script type="math/tex">q(x)</script>的极小点作为下一个迭代点<script type="math/tex">x^{(k+1)}</script>，可以看到<script type="math/tex">x^{(k+1)}</script>处目标函数值比<script type="math/tex">x^{(k)}</script>处的目标函数值要小，如此循环往复，不断迭代，最终可以找到目标函数<script type="math/tex">f(x)</script>的极小点。</p>
<p>我们现在的目标是求一元单值函数<script type="math/tex">f(x)</script>在区间上的极小点，使用牛顿法求解时要求<script type="math/tex">f(x)</script>连续二阶可微，即迭代点<script type="math/tex">x^{(k)}</script>处的<script type="math/tex">f(x^{(k)})</script>、<script type="math/tex">f'(x^{(k)})</script>和<script type="math/tex">f''(x^{(k)})</script>均可求得。</p>
<p>在迭代点<script type="math/tex">x^{(k)}</script>处对目标函数<script type="math/tex">f(x)</script>做Taylor展开，保留前两项，可得<script type="math/tex">f(x)</script>在<script type="math/tex">x^{(k)}</script>处的二阶近似：$$q(x)=f(x^{(k)})+f'(x^{(k)})(x-x^{(k)})+\frac{1}{2}f''(x^{(k)})(x-x^{(k)})^2$$</p>
<p>求<script type="math/tex">q(x)</script>的极小点，即求<script type="math/tex">q'(x)=0</script>，可得：$$q'(x)=f'(x^{(k)})+f''(x^{(k)})(x-x^{(k)})=0$$</p>
<p>则下一个迭代点<script type="math/tex">x^{(k+1)}</script>作为<script type="math/tex">q(x)</script>的极小点，即为上式的解，有：$$x^{(k+1)}=x^{(k)}-\frac{f'(x^{(k)})}{f''(x^{(k)})}$$</p>
<p>上式即为牛顿法的迭代公式，不断产生试探点列直到找到极小值。</p>
<p>需要注意的是，当<script type="math/tex">f''(x)>0</script>对于区间内所有的<script type="math/tex">x</script>都成立时，牛顿法能够正常运行。如果在某些点处<script type="math/tex">f''(x)<0</script>，牛顿法可能收敛到极大点，如下图所示：</p>
<p><img src="/img/NLP/LS_Nerton_max.jpg" srcset="/img/loading.gif" lazyload alt="LS_Nerton_max" style="zoom:25%;" /></p>
<h4 id="6-3-2-二分法"><a href="#6-3-2-二分法" class="headerlink" title="6.3.2 二分法"></a>6.3.2 二分法</h4><p>二分法的思想非常简单，仅使用了目标函数的一阶导数来压缩区间。二分法要求目标函数<script type="math/tex">f</script>在初始区间<script type="math/tex">[a,b]</script>为单谷函数。二分法的过程如下：</p>
<p>首先确定初始区间的中点作为第一个迭代点<script type="math/tex">x^{(0)}=(a+b)/2</script>。然后计算函数<script type="math/tex">f</script>在<script type="math/tex">x^{(0)}</script>处的一阶导数<script type="math/tex">f'(x^{(0)})</script>。如果一阶导数<script type="math/tex">f'(x^{(0)})>0</script>，说明极小点位于<script type="math/tex">x^{(0)}</script>的左侧，也就是意味着极小点所在的区间可以压缩为<script type="math/tex">[a,x^{(0)}]</script>；反之，如果一阶导数<script type="math/tex">f'(x^{(0)})<0</script>，说明极小点位于<script type="math/tex">x^{(0)}</script>的右侧，也就是意味着极小点所在的区间可以压缩为<script type="math/tex">[x^{(0)},b]</script>。最后，如果<script type="math/tex">f'(x^{(0)})=0</script>，说明<script type="math/tex">x^{(0)}</script>就是极小点，搜索停止。</p>
<p>按照上述方式，之后每次迭代中都可以得到一个新的区间，然后再根据区间中点处一阶导数的符号进一步将区间压缩为左半区间或右半区间。所以说，在每次迭代中，区间的压缩比为<script type="math/tex">\frac{1}{2}</script>。因此在<script type="math/tex">N</script>次迭代后整个区间的压缩比为<script type="math/tex">(\frac{1}{2})^N</script>，接下来可以看到，这一总压缩比比黄金分割法和斐波那契数列法的总压缩比都要小。</p>
<h4 id="6-3-3-黄金分割法"><a href="#6-3-3-黄金分割法" class="headerlink" title="6.3.3 黄金分割法"></a>6.3.3 黄金分割法</h4><p>黄金分割法和接下来要讨论的斐波那契数列法都属于区间压缩算法，翟乔柱老师的PPT中有如下论述：</p>
<p><img src="/img/NLP/LS_区间压缩.jpg" srcset="/img/loading.gif" lazyload alt="LS_区间压缩" style="zoom:25%;" /></p>
<p>区间压缩算法的核心就在于如何确定<script type="math/tex">t_{l+1}</script>，主要有两种思路：</p>
<ul>
<li>保持区间缩小比：即为本小节要讨论的黄金分割法</li>
<li>使最终区间最短：即为下一小节要讨论的斐波那契数列法</li>
</ul>
<p>先来讨论黄金分割法，这里直接放上翟桥柱老师的课件：</p>
<center class="half">
    <img src="/img/NLP/LS_黄金分割法1.jpg" srcset="/img/loading.gif" lazyload width="300"/>
    <img src="/img/NLP/LS_黄金分割法2.jpg" srcset="/img/loading.gif" lazyload width="300"/>
</center>


<p>注意黄金分割法的三个核心前提为：</p>
<ul>
<li>对称压缩：<script type="math/tex">a_1-a_0=b_0-b_1=\rho(b_0-a_0)</script>，其中<script type="math/tex">\rho<\frac{1}{2}</script></li>
<li>每一步压缩比相同：<script type="math/tex">\rho</script>在每一步中不变，为一常数</li>
<li>每次迭代只需计算一个试探点处的目标函数值：若当前迭代选择了右边区间作为下一次的区间，则下一次迭代时左边的试探点应和这一次右边的试探点重合；反之亦然。</li>
</ul>
<p><img src="/img/NLP/LS_黄金分割法rou.jpg" srcset="/img/loading.gif" lazyload alt="LS_黄金分割法rou" style="zoom:25%;" /></p>
<p>如上图所示，假设区间<script type="math/tex">[a_0,b_0]</script>的长度为1，则<script type="math/tex">[a_0,a_1]</script>和<script type="math/tex">[b_1,b_0]</script>的长度均为<script type="math/tex">\rho</script>，若选择了区间<script type="math/tex">[a_0,b_1]</script>作为下一次迭代的初始区间，则区间长度为<script type="math/tex">1-\rho</script>，现在要求<script type="math/tex">a_1</script>和<script type="math/tex">b_2</script>重合，以减少一次试探点处目标函数值的计算。由于<script type="math/tex">[b_2,b_1]</script>的长度为<script type="math/tex">\rho(b_1-a_0)=\rho(1-\rho)</script>，<script type="math/tex">[a_1,b_1]</script>的长度为<script type="math/tex">1-(a_1-a_0)-(b_0-b_1)=1-2\rho</script>。所以说有：$$\rho(1-\rho)=1-2\rho$$</p>
<p>可以解得：                                                              $$\rho=\frac{3-\sqrt{5}}{2}$$</p>
<p>可以看到，黄金分割法的区间压缩比为<script type="math/tex">1-\rho=0.618</script>，经过<script type="math/tex">N</script>次迭代压缩之后，极小点所在区间长度将压缩到初始区间长度的<script type="math/tex">(0.618)^N</script>，为黄金分割法的总压缩比。</p>
<h4 id="6-3-4-斐波那契数列法"><a href="#6-3-4-斐波那契数列法" class="headerlink" title="6.3.4 斐波那契数列法"></a>6.3.4 斐波那契数列法</h4><p>在黄金分割法压缩区间的过程中，每一步的压缩比始终不变。如果允许在区间压缩迭代的每一步对压缩比进行动态的调整，并将总压缩比最小作为优化的目标，就得到了斐波那契数列法。</p>
<p>斐波那契数列法和黄金分割法有类似的前提，只是允许每一步的压缩比可以动态调整：</p>
<ul>
<li><p>对称压缩：<script type="math/tex">a_{k+1}-a_k=b_k-b_{k+1}=\rho_k(b_k-a_k)</script>，其中<script type="math/tex">\rho_k<\frac{1}{2}</script></p>
</li>
<li><p>每次迭代只需计算一个试探点处的目标函数值：<script type="math/tex">\rho_{k+1}(1-\rho_{k})=1-2\rho_{k}</script>，即满足约束：<script type="math/tex">\rho_{k+1}=1-\frac{\rho_k}{1-\rho_k}</script></p>
<p><img src="/img/NLP/LS_斐波那契数列法rou.jpg" srcset="/img/loading.gif" lazyload alt="LS_斐波那契数列法rou" style="zoom:20%;" /></p>
</li>
</ul>
<p>存在很多组序列<script type="math/tex">\rho_1,\rho_2,...</script>可以满足上述需求，一个合理的想法是，我们要找到一组满足上述要求的序列<script type="math/tex">\rho_1,\rho_2,...</script>，使得<script type="math/tex">N</script>次压缩的总压缩比<script type="math/tex">(1-\rho_1)(1-\rho_2)\cdot\cdot\cdot\cdot(1-\rho_N)</script>最小。</p>
<p>这可以使用下面的有约束优化问题来描述：</p>
<p>​                                                   $$minimize  \ \ \ \ \ \ \ \ (1-\rho_1)(1-\rho_2)\cdot\cdot\cdot\cdot(1-\rho_N)
$$   $$ subject \ to  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \rho_{k+1}=1-\frac{\rho_k}{1-\rho_k}$$   $$ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 0\le\rho_k\le\frac{1}{2}$$              </p>
<p>上述优化问题的解为：$$\rho_1=1-\frac{F_N}{F_{N+1}}\ ,\ \rho_2=1-\frac{F_{N-1}}{F_{N}}\ , \ ...\ , \ \rho_N=1-\frac{F_1}{F_{2}}$$</p>
<p>其中<script type="math/tex">F_n</script>为斐波那契数列的第<script type="math/tex">n</script>项，这也是为什么其被称作斐波那契数列法。</p>
<p>斐波那契数列的总压缩比为<script type="math/tex">\frac{1}{F_{N+1}}</script>。</p>
<p>翟桥柱老师的课件中提供了一种很有启发性的思路，摘录如下：</p>
<center class="half">
    <img src="/img/NLP/LS_斐波那契数列2.jpg" srcset="/img/loading.gif" lazyload width="300"/>
    <img src="/img/NLP/LS_斐波那契数列法1.jpg" srcset="/img/loading.gif" lazyload width="300"/>
</center>


<h3 id="6-4-非精确一维搜索"><a href="#6-4-非精确一维搜索" class="headerlink" title="6.4 非精确一维搜索"></a>6.4 非精确一维搜索</h3><p>在用于非线性规划定步长子问题时，精确的一维搜索通常存在一些问题：</p>
<ul>
<li>精确的求解极小点需要非常大的计算量，甚至在某些情况下极小点根本不存在</li>
<li>应该将更多的计算资源配置在多维优化算法而不是追求高精度的一维搜索上</li>
</ul>
<p>这些缺点意味着应该为一维搜索算法设计合适的停止条件，当步长为满意步长时即停止搜索；并使得即使一维搜索结果精度偏低，仍然能够保证目标函数值在两次迭代中得到足够程度的下降，一个基本的理念是：步长不要太小也不要太大。因此，非精确一维搜索应运而生。</p>
<p>非精确一维搜索方法并不试图寻找一维搜索问题的全局或局部最优解，而是致力于获得一个可以是目标函数下降的满意解。因此，非精确一维搜索主要包含两个结构要素：</p>
<ul>
<li>满意步长的判别准则：主要有三种常用的满意步长准则，Goldstein准则、Armijo准则和Wolfe准则</li>
<li>获得满意步长的方法：具体根据每种满意步长准则都有响应的算法</li>
</ul>
<h4 id="6-4-1-Goldstein准则"><a href="#6-4-1-Goldstein准则" class="headerlink" title="6.4.1 Goldstein准则"></a>6.4.1 Goldstein准则</h4><p><img src="/img/NLP/LS_Goldstein.jpg" srcset="/img/loading.gif" lazyload alt="LS_Goldstein" style="zoom:20%;" /></p>
<h4 id="6-4-2-Armijo准则"><a href="#6-4-2-Armijo准则" class="headerlink" title="6.4.2 Armijo准则"></a>6.4.2 Armijo准则</h4><p><img src="/img/NLP/LS_Armijo.jpg" srcset="/img/loading.gif" lazyload alt="LS_Armijo" style="zoom:20%;" /></p>
<h4 id="6-4-3-Wolfe准则"><a href="#6-4-3-Wolfe准则" class="headerlink" title="6.4.3 Wolfe准则"></a>6.4.3 Wolfe准则</h4><p><img src="/img/NLP/LS_Wolfe.jpg" srcset="/img/loading.gif" lazyload alt="LS_Wolfe" style="zoom:20%;" /></p>
<h4 id="6-4-4-非精确一维搜索算法"><a href="#6-4-4-非精确一维搜索算法" class="headerlink" title="6.4.4 非精确一维搜索算法"></a>6.4.4 非精确一维搜索算法</h4><p>包括Goldstein准则定步长算法、Armijo划界法等等。</p>
<h2 id="7-搜索方向：算法的核心"><a href="#7-搜索方向：算法的核心" class="headerlink" title="7 搜索方向：算法的核心"></a><strong>7 搜索方向：算法的核心</strong></h2><h3 id="7-1-无约束优化"><a href="#7-1-无约束优化" class="headerlink" title="7.1 无约束优化"></a>7.1 无约束优化</h3><p>无约束优化问题主要算法的特点如下：</p>
<p><img src="/img/NLP/无约束优化问题概览.jpg" srcset="/img/loading.gif" lazyload alt="无约束优化问题概览" style="zoom:28%;" /></p>
<p>上图中的算法都会在下面得到介绍。</p>
<h4 id="7-1-1-梯度方法"><a href="#7-1-1-梯度方法" class="headerlink" title="7.1.1 梯度方法"></a>7.1.1 梯度方法</h4><h5 id="7-1-1-1-梯度方法概述"><a href="#7-1-1-1-梯度方法概述" class="headerlink" title="7.1.1.1 梯度方法概述"></a>7.1.1.1 梯度方法概述</h5><p>梯度方法主要基于关于梯度和下降方向的这样几个事实：</p>
<ul>
<li>与负梯度方向夹角小于90°的方向是下降方向</li>
<li>负梯度方向是最容易获得的下降方向，也是函数值下降最快的方向</li>
<li>可以用当前搜索点的梯度是否接近于0作为搜索的终止准则之一</li>
</ul>
<p>则梯度方法选择负梯度方向<script type="math/tex">-\nabla f(\boldsymbol{x}^{(k)})</script>作为每次迭代时的搜索方向，迭代方程如下式，其中步长<script type="math/tex">\alpha_k>0</script>： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_k\nabla f(\boldsymbol{x}^{(k)})$$</p>
<p>在搜索过程中，目标函数的梯度值不断变化。可以设定很小的步长，每次迭代都重新计算梯度，也可以设置很大的步长。前者的工作量非常大，后者则容易在极小点附近产生锯齿状的收敛路径，优势在于梯度的计算次数少一些。</p>
<p>梯度方法包括很多种具体的算法，区别主要在每一次步长<script type="math/tex">\alpha_k</script>的选取上：</p>
<ul>
<li>最速下降法：采用精确一维搜索找到最优的步长：<script type="math/tex">\alpha_k=\underset{\alpha_k>0}{argmin}\ \ f(\boldsymbol{x}^{(k)}-\alpha_k\nabla f(\boldsymbol{x}^{(k)})</script></li>
<li>步长固定梯度法：每次迭代的步长固定为一个常数<script type="math/tex">\alpha</script></li>
<li>BB方法：<script type="math/tex">\alpha_k=\frac{(\boldsymbol{s}^{(k)})^T\boldsymbol{s}^{(k)}}{(\boldsymbol{s}^{(k)})^T\boldsymbol{y}^{(k)}}</script>，其中<script type="math/tex">\boldsymbol{s}^{(k)}=\boldsymbol{x}^{(k)}-\boldsymbol{x}^{(k-1)},\boldsymbol{y}^{(k)}=\nabla f(\boldsymbol{x}^{(k)})-\nabla f(\boldsymbol{x}^{(k-1)})</script>，该步长是对Hessian矩阵的一个近似，可以认为是伪二阶方法，通常结合一维搜索使用。</li>
</ul>
<h5 id="7-1-1-2-最速下降法"><a href="#7-1-1-2-最速下降法" class="headerlink" title="7.1.1.2 最速下降法"></a>7.1.1.2 最速下降法</h5><p>接下来主要讨论一下最速下降法的特点：</p>
<p>最速下降法的核心理念为：在每次迭代中选择合适的步长，使目标函数值能够得到最大程度的减小，即在每次迭代中都选择<script type="math/tex">\alpha_k=\underset{\alpha_k>0}{argmin}\ \ f(\boldsymbol{x}^{(k)}-\alpha_k\nabla f(\boldsymbol{x}^{(k)})</script>，即采用精确一维搜索确定每一步的步长。</p>
<p>由于在每一步都采用精确一维搜索得到了使得目标函数下降最多的步长，在最速下降法中有一个很有意思的结论：</p>
<p><strong>最速下降法的相邻搜索方向是正交的。</strong>如下图所示：</p>
<p><img src="/img/NLP/最速下降法搜索方向正交.jpg" srcset="/img/loading.gif" lazyload alt="最速下降法搜索方向正交" style="zoom:25%;" /></p>
<p>这个结论通过步长的选择方法<script type="math/tex">\alpha_k=\underset{\alpha_k>0}{argmin}\ \ f(\boldsymbol{x}^{(k)}-\alpha_k\nabla f(\boldsymbol{x}^{(k)})</script>可以很容易的得到：</p>
<p>注意到<script type="math/tex">\alpha_k</script>是函数<script type="math/tex">\phi(\alpha)=f(\boldsymbol{x}^{(k)}-\alpha\nabla f(\boldsymbol{x}^{(k)})</script>的极小点，利用局部极小点的必要条件和求导的链式法则，有：$$\frac{d\phi_k(\alpha_k)}{d\alpha_k}=(\nabla f(\boldsymbol{x}^{(k)}-\alpha_k\nabla f(\boldsymbol{x}^{(k)}))^T(-\nabla f(\boldsymbol{x}^{(k)}))=-(\nabla f(\boldsymbol{x}^{(k+1)})^T(\nabla f(\boldsymbol{x}^{(k)}))=0$$</p>
<p>可以看到<script type="math/tex">\nabla f(\boldsymbol{x}^{(k)})</script>和<script type="math/tex">\nabla f(\boldsymbol{x}^{(k+1)})</script>内积为0，即两次搜索方向相互正交。</p>
<p>相邻搜索方向正交是一个非常不好的特性，因为搜索并不是直接冲着极小值去的，而是一个”拉锯”的现象，被称作Zigzag现象，可能收敛很慢。在上运筹学课程时，老师提出了一个思考题：不用精确一维搜索，是否有助于避免Zigzag现象？通过上面证明，可以很轻松的给出回答，相邻方向正交这个结论严格依赖于<script type="math/tex">\alpha_k</script>是函数<script type="math/tex">\phi(\alpha)=f(\boldsymbol{x}^{(k)}-\alpha\nabla f(\boldsymbol{x}^{(k)})</script>的极小点这个条件，如果不采用精确一维搜索，则<script type="math/tex">\nabla f(\boldsymbol{x}^{(k)})</script>和<script type="math/tex">\nabla f(\boldsymbol{x}^{(k+1)})</script>的内积不为0，相邻搜索方向也就不正交，有助于避免Zigzag现象。</p>
<h5 id="7-1-1-3-梯度方法的收敛性和收敛率"><a href="#7-1-1-3-梯度方法的收敛性和收敛率" class="headerlink" title="7.1.1.3 梯度方法的收敛性和收敛率"></a>7.1.1.3 梯度方法的收敛性和收敛率</h5><p>为了分析算法的收敛特性，需要将目标函数设定为二次型函数：$$f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x}-\boldsymbol{b}^T\boldsymbol{x}$$</p>
<p>首先展示两个关于梯度方法收敛性的结论：</p>
<ul>
<li>对于最速下降法，对于任意的初始点<script type="math/tex">\boldsymbol{x}^{(0)}</script>，都有<script type="math/tex">\boldsymbol{x}^{(k)}\to\boldsymbol{x}^*</script></li>
<li>对于步长固定梯度法，当且仅当步长<script type="math/tex">0<\alpha<\frac{2}{\lambda_{max}(Q)}</script>时，有<script type="math/tex">\boldsymbol{x}^{(k)}\to\boldsymbol{x}^*</script></li>
</ul>
<p>即在目标函数是二次型函数时，最速下降法始终时全局收敛的，而步长固定梯度法只有在步长较小时才有全局收敛的特性，这让我想到了在神经网络中学习率较小时有助于收敛这一事实。</p>
<p>讨论完了算法的收敛性，接下来展示一下收敛的速度，及收敛率的概念：</p>
<ul>
<li><p>对于最速下降法，有结论：<script type="math/tex">f(\boldsymbol{x}^{(k+1)})\le (\frac{\lambda_{max}(Q)-\lambda_{min}(Q)}{\lambda_{max}(Q)+\lambda_{min}(Q)})^2 f(\boldsymbol{x}^{(k)})</script>。</p>
<p>所以说，当矩阵<script type="math/tex">Q</script>的最大特征值和最小特征值相差很大时收敛很慢。</p>
<p>从下图中也可以看出，左图中最大特征值和最小特征值相等，收敛很快；右图中最大特征值和最小特征值相差很大，最速下降法收敛很慢。</p>
<center class="half">
    <img src="/img/NLP/最速下降法收敛率1.jpg" srcset="/img/loading.gif" lazyload width="200"/>
    <img src="/img/NLP/最速下降法收敛率2.jpg" srcset="/img/loading.gif" lazyload width="300"/>
</center>
</li>
<li><p>最速下降法在最坏情况下的收敛阶数为1。</p>
</li>
</ul>
<h4 id="7-1-2-牛顿法"><a href="#7-1-2-牛顿法" class="headerlink" title="7.1.2 牛顿法"></a>7.1.2 牛顿法</h4><h5 id="7-1-2-1-牛顿法引言"><a href="#7-1-2-1-牛顿法引言" class="headerlink" title="7.1.2.1 牛顿法引言"></a>7.1.2.1 牛顿法引言</h5><p>在确定搜索方向时，最速下降法只用到了目标函数的一阶导数也即梯度。这种方式并非非常高效，可能收敛较慢，如果能在迭代方法中引入高阶导数，效率可能会优于最速下降法。牛顿法就是如此，同时使用一阶和二阶导数来确定搜索方向。当初始点与目标函数极小点足够接近时，牛顿法的效率确实优于最速下降法。</p>
<p>牛顿法的大体思路为：</p>
<p><img src="/img/NLP/Newton_mthd.jpg" srcset="/img/loading.gif" lazyload alt="Newton_mthd" style="zoom:30%;" /></p>
<p>如上图所示，给定一个迭代点之后，首先构造一个二次型函数，其与目标函数在该点处的一阶和二阶导数相等，以此作为目标函数的近似表达式，之后求该二次型函数的极小点，作为下一次迭代的起始点。</p>
<p>将函数<script type="math/tex">f(\boldsymbol{x})</script>在点<script type="math/tex">\boldsymbol{x}^{(k)}</script>处进行Taylor展开，保留两项：$$f(\boldsymbol{x})\approx f(\boldsymbol{x}^{(k)})+(\boldsymbol{x}-\boldsymbol{x}^{(k)})^T\boldsymbol{g}^{(k)}+\frac{1}{2}(\boldsymbol{x}-\boldsymbol{x}^{(k)})^T\boldsymbol{F}(\boldsymbol{x}^{(k)})(\boldsymbol{x}-\boldsymbol{x}^{(k)})=q(\boldsymbol{x})$$</p>
<p>其中，<script type="math/tex">\boldsymbol{g}^{(k)}=\nabla f(\boldsymbol{x}^{(k)})</script>，<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})=\nabla ^2f(\boldsymbol{x}^{(k)})</script>。</p>
<p>为求得函数<script type="math/tex">q(\boldsymbol{x})</script>的极小点，将局部极小点的一阶必要条件应用到函数<script type="math/tex">q(\boldsymbol{x})</script>：$$\nabla q(\boldsymbol{x})=\boldsymbol{g}^{(k)}+\boldsymbol{F}(\boldsymbol{x}^{(k)})(\boldsymbol{x}-\boldsymbol{x}^{(k)})=\boldsymbol{0}$$</p>
<p>如果<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})>0</script>，则函数<script type="math/tex">q(\boldsymbol{x})</script>的极小点为： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}$$</p>
<p>这就是最基础的牛顿法的迭代公式。</p>
<p>由于在实际中我们从来不会直接对矩阵求逆，所以在一次迭代中，牛顿法可以分成两步：</p>
<ul>
<li>求解<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})\boldsymbol{d}^{(k)}=-\boldsymbol{g}^{(k)}</script>，得到<script type="math/tex">\boldsymbol{d}^{(k)}</script></li>
<li>确定下一个迭代点<script type="math/tex">\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+\boldsymbol{d}^{(k)}</script></li>
</ul>
<p>第一步需要求解一个<script type="math/tex">n</script>维的线性非齐次方程组，所以在之后会设计更加高效的算法提高牛顿法的实用程度。</p>
<h5 id="7-1-2-2-牛顿法性质分析"><a href="#7-1-2-2-牛顿法性质分析" class="headerlink" title="7.1.2.2 牛顿法性质分析"></a>7.1.2.2 牛顿法性质分析</h5><p>最基本的牛顿法存在的最大问题是，当目标函数的Hessian矩阵<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})</script>非正定时，牛顿法确定的搜索方向并不一定是目标函数值下降的方向。甚至在某些情况，比如说初始点远离目标函数极小点时，即使Hessian矩阵正定，牛顿法也不具有下降特性。但牛顿法的优势是，如果初始点离极小点比较近，那么牛顿法将表现出相当好的收敛性。</p>
<p>对于牛顿法的收敛性，有以下两个结论：</p>
<ul>
<li>目标函数为二次型函数时，对于任意初始点，牛顿法的收敛阶数为<script type="math/tex">\infty</script></li>
<li>对于一般形式的目标函数，对于所有与极小点足够接近的初始点，牛顿法能够正常运行，且收敛率至少为<script type="math/tex">2</script></li>
</ul>
<p>但是，如果初始点离极小点较远，则牛顿法不一定收敛。而且在求解最小化问题时，该方法不一定具有下降特性。幸运的是，可以对牛顿法做一些修正，使其保持下降特性。</p>
<p>下面的定理说明了这一点是可行的：</p>
<p>一个重要定理：对于迭代点<script type="math/tex">\boldsymbol{x}^{(k)}</script>，若Hessian矩阵<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})</script>正定，牛顿法确定的方向<script type="math/tex">-\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}</script>是下降方向。</p>
<p>所以我们可以对原始的迭代公式做如下修正： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_k\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}$$</p>
<p>其中：$$\alpha_k=\underset{\alpha_k\ge 0}{argmin}\ \ f(\boldsymbol{x}^{(k)}-\alpha_k\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)})$$</p>
<p>也就是每次迭代时都在方向<script type="math/tex">-\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}</script>上开展一次步长，由此确定每次搜素的步长，以保证牛顿法在每次迭代时具有下降特性。</p>
<h5 id="7-1-2-3-牛顿法的Levenberg-Marquardt修正"><a href="#7-1-2-3-牛顿法的Levenberg-Marquardt修正" class="headerlink" title="7.1.2.3 牛顿法的Levenberg-Marquardt修正"></a>7.1.2.3 牛顿法的Levenberg-Marquardt修正</h5><p>如果在迭代过程中Hessian矩阵不正定，则牛顿法确定的方向<script type="math/tex">-\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}</script>不一定是下降方向。所以为了解决Hessian矩阵可能不正定的问题，可以进行如下的Levenberg-Marquardt修正： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_k(\boldsymbol{F}(\boldsymbol{x}^{(k)})+\mu_k\boldsymbol{I})^{-1}\boldsymbol{g}^{(k)}$$</p>
<p>可以证明，只要<script type="math/tex">\mu_k</script>足够大，总可以保证矩阵<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})+\mu_k\boldsymbol{I}</script>的特征值均为正数，即矩阵<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})+\mu_k\boldsymbol{I}</script>正定，此时可以保证搜索方向<script type="math/tex">-(\boldsymbol{F}(\boldsymbol{x}^{(k)})+\mu_k\boldsymbol{I})^{-1}\boldsymbol{g}^{(k)}</script>是一个下降方向。在此方向上引入一次一维搜索，就可以保证每次迭代中目标函数的下降特性。</p>
<p>令<script type="math/tex">\mu_k\to 0</script>，牛顿法的Levenberg-Marquardt修正就可以逐步接近牛顿法；令<script type="math/tex">\mu_k\to\infty</script>，Levenberg-Marquardt修正又会表现出步长较小时梯度方法的特性。实际应用中，一开始可以为<script type="math/tex">\mu_k</script>选择较小的值，然后逐渐缓慢增加，直到出现下降特性，即<script type="math/tex">f(\boldsymbol{x}^{(k+1)})<f(\boldsymbol{x}^{(k)})</script>。</p>
<h4 id="7-1-3-共轭方向法"><a href="#7-1-3-共轭方向法" class="headerlink" title="7.1.3 共轭方向法"></a>7.1.3 共轭方向法</h4><h5 id="7-1-3-1-共轭方向法引言"><a href="#7-1-3-1-共轭方向法引言" class="headerlink" title="7.1.3.1 共轭方向法引言"></a>7.1.3.1 共轭方向法引言</h5><p>从上面可以看到，牛顿法收敛速度较快，但是每一次迭代都需要求解一个<script type="math/tex">n</script>维线性方程组，需要的计算量和计算时间很长，共轭方法可以仅利用Hessian矩阵的部分信息确定每一次的搜索方向，计算速度大大提高。从计算效率来看，共轭方向法位于最速下降法和牛顿法之间。</p>
<p>共轭方向法具有以下特点：</p>
<ul>
<li>对于<script type="math/tex">n</script>维二次型问题，可以在<script type="math/tex">n</script>步之内得到结果。</li>
<li>作为共轭方向法的典型代表，共轭梯度法不需要计算Hessian矩阵。</li>
<li>不需要储存<script type="math/tex">n\times n</script>的矩阵，也不需要对其求逆。</li>
</ul>
<p>对于不同的迭代算法，影响其效率的关键因素为每次迭代的搜索方向。</p>
<p>共轭方向法之所以重要是由于有以下性质存在：</p>
<p>对于一个<script type="math/tex">n</script>变量的二次型函数<script type="math/tex">f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x}-\boldsymbol{b}^T\boldsymbol{x},Q=Q^T>0</script>来说，最好的搜索方向为共轭方向。</p>
<p>一些基本定义为：</p>
<p>共轭方向的定义为：如果<script type="math/tex">\mathbb{R}^n</script>中的两个方向<script type="math/tex">\boldsymbol{d}^{(1)}</script>和<script type="math/tex">\boldsymbol{d}^{(2)}</script>满足<script type="math/tex">{\boldsymbol{d}^{(1)}}^T Q\boldsymbol{d}^{(2)}</script>，则称它们关于<script type="math/tex">Q</script>是共轭的。</p>
<p>如果一组方向向量中，任意两个不同的向量都关于<script type="math/tex">Q</script>共轭，则称这向量组关于<script type="math/tex">Q</script>共轭，且这些向量线性无关。</p>
<h5 id="7-1-3-2-基本的共轭方向法"><a href="#7-1-3-2-基本的共轭方向法" class="headerlink" title="7.1.3.2 基本的共轭方向法"></a>7.1.3.2 基本的共轭方向法</h5><p>最基本的共轭方向法是针对<script type="math/tex">n</script>维二次型函数的最小化问题： $$f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x}-\boldsymbol{b}^T\boldsymbol{x},Q=Q^T>0$$</p>
<p>则算法具体为：</p>
<p>给定初始点和一组关于<script type="math/tex">Q</script>共轭的方向<script type="math/tex">\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(n-1)}</script>，迭代公式为：$$\boldsymbol{g}^{(k)}=\nabla f(\boldsymbol{x}^{(k)})=Q\boldsymbol{x}^{(k)}-\boldsymbol{b}$$ $$\alpha_k=-\frac{{\boldsymbol{g}^{(k)}}^T\boldsymbol{d}^{(k)}}{{\boldsymbol{d}^{(k)}}^TQ\boldsymbol{d}^{(k)}}$$ $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}$$</p>
<p>上述基本的共轭方向法有一些很有意思的性质：</p>
<ul>
<li><p>对于任意的初始点，基本的共轭方向算法都能在<script type="math/tex">n</script>次迭代之内收敛到唯一的全局极小点。</p>
</li>
<li><p>对于所有的<script type="math/tex">k</script>，都有： $${\boldsymbol{g}^{(k+1)}}^T\boldsymbol{d}^{(k)}=0$$</p>
<p>这意味着对于每一次迭代时的步长<script type="math/tex">\alpha_k</script>都有： $$\alpha_k={argmin}\ \ f(\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)})$$</p>
<p>还有更一般的结论：对于所有<script type="math/tex">0\le k\le n-1,0\le i \le k</script>，都有： $${\boldsymbol{g}^{(k+1)}}^T\boldsymbol{d}^{(i)}=0$$</p>
<p>这意味着向量<script type="math/tex">\boldsymbol{g}^{(k)}</script>正交于由向量<script type="math/tex">\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(k)}</script>张成的子空间中的任意向量，如下图所示：</p>
<p><img src="/img/NLP/共轭梯度法_垂直于子空间.jpg" srcset="/img/loading.gif" lazyload alt="共轭梯度法_垂直于子空间" style="zoom:25%;" /></p>
<p>可以看出，向量<script type="math/tex">\boldsymbol{g}^{(k)}</script>正交于由向量<script type="math/tex">\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(k)}</script>张成的子空间 。  </p>
</li>
<li><p>迭代时函数值<script type="math/tex">f(\boldsymbol{x}^{(k+1)})</script>不仅能满足<script type="math/tex">f(\boldsymbol{x}^{(k+1)})=\min_{\alpha_k} f(\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)})</script>，还能满足：$$f(\boldsymbol{x}^{(k+1)})=\underset{\alpha_0,...,\alpha_k}{\min }f(\boldsymbol{x}^{(0)}+\sum_{i=0}^k\alpha_i\boldsymbol{d}^{(i)})$$</p>
<p>这说明<script type="math/tex">f(\boldsymbol{x}^{(k+1)})</script>是目标函数在子空间<script type="math/tex">span[\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(k)}]</script>中的极小值。随着迭代不断进行，<script type="math/tex">k</script>不断增大，子空间<script type="math/tex">span[\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(k)}]</script>不断扩张，直到充满整个<script type="math/tex">\mathbb{R}^n</script>（由共轭方向的定义，<script type="math/tex">\boldsymbol{d}^{(0)},\boldsymbol{d}^{(1)},...,\boldsymbol{d}^{(n)}</script>是线性无关的）。所以说，最多迭代<script type="math/tex">n</script>次，不断扩张的子空间即可包含原目标函数的极小点。这也就是为什么对于<script type="math/tex">n</script>维二次型问题，共轭方向法可以在<script type="math/tex">n</script>步之内得到结果。</p>
</li>
</ul>
<h5 id="7-1-3-3-共轭梯度法"><a href="#7-1-3-3-共轭梯度法" class="headerlink" title="7.1.3.3 共轭梯度法"></a>7.1.3.3 共轭梯度法</h5><p>基本的共轭方向法效率很高，但是前提是必须事先给定一组<script type="math/tex">Q</script>共轭方向。在共轭梯度法中，不需要提前给定<script type="math/tex">Q</script>共轭方向，而是随着迭代的进行不断产生<script type="math/tex">Q</script>共轭方向。在每次迭代中，利用上一个搜索方向和目标函数在当前点的梯度向量之间的线性组合构造一个新方向，使之与前面已经产生的搜索方向组成<script type="math/tex">Q</script>共轭方向。</p>
<p><strong>二次型问题中的共轭梯度法：</strong>  $$f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x}-\boldsymbol{b}^T\boldsymbol{x},Q=Q^T>0$$</p>
<p>初始点为<script type="math/tex">\boldsymbol{x}^{(0)}</script>，第一次迭代时搜素方向采用最速下降法的方向：   $$\boldsymbol{d}^{(0)}=-\boldsymbol{g}^{(0)}$$</p>
<p>产生第一个迭代点：                  $$\boldsymbol{x}^{(1)}=\boldsymbol{x}^{(0)}+\alpha_0\boldsymbol{d}^{(0)}$$</p>
<p>其中步长为：  $$\alpha_0=\underset{\alpha_0\ge 0}{argmin}\ \ f(\boldsymbol{x}^{(0)}+\alpha_0\boldsymbol{d}^{(0)})=\alpha_k=-\frac{{\boldsymbol{g}^{(0)}}^T\boldsymbol{d}^{(0)}}{{\boldsymbol{d}^{(0)}}^TQ\boldsymbol{d}^{(0)}}$$</p>
<p>之后在第<script type="math/tex">k+1</script>次迭代时，可将搜索方向<script type="math/tex">\boldsymbol{d}^{(k+1)}</script>写为当前的梯度<script type="math/tex">\boldsymbol{g}^{(k+1)}</script>和上一次搜索方向<script type="math/tex">\boldsymbol{d}^{(k)}</script>之间的线性组合：$$\boldsymbol{d}^{(k+1)}=-\boldsymbol{g}^{(k+1)}+\beta_k\boldsymbol{d}^{(k)}$$</p>
<p>按照如下方式选择<script type="math/tex">\beta_k</script>，可以使每次的搜索方向和之前搜索方向组成的向量组组成<script type="math/tex">Q</script>共轭方向：  $$\beta_k=-\frac{{\boldsymbol{g}^{(k+1)}}^TQ\boldsymbol{d}^{(k)}}{{\boldsymbol{d}^{(k)}}^TQ\boldsymbol{d}^{(k)}}$$</p>
<p><strong>非二次型问题中的共轭梯度法：</strong></p>
<p>在实际中，我们还会遇到目标函数为非二次型函数的优化问题，如果将二次型函数<script type="math/tex">f(\boldsymbol{x})=\boldsymbol{x}^TQ\boldsymbol{x}-\boldsymbol{b}^T\boldsymbol{x}</script>视为目标函数Taylor展开式的二阶近似，就可以将共轭梯度法推广至一般的非线性目标函数。如果展开点离目标函数极小点较近，则二次型函数可以提供很好的近似。</p>
<p>在二次型问题中，目标函数的Hessian矩阵<script type="math/tex">Q</script>为常数矩阵，但是对于一般的非线性函数，每次迭代时都必须重新计算Hessian矩阵，需要非常大的计算量。因此在将共轭梯度法推广到非二次型问题的时候，核心就在于消除每次迭代中求Hessian矩阵的环节，即利用当前迭代点的函数值和梯度值的组合代替计算公式中的Hessian矩阵。</p>
<p>二次型问题中，Hessian矩阵<script type="math/tex">Q</script>只出现在<script type="math/tex">\alpha_k</script>和<script type="math/tex">\beta_k</script>的计算过程中。由于<script type="math/tex">\alpha_k</script>可以由一维搜索过程替代，因此只需要从<script type="math/tex">\beta_k</script>的计算过程中去掉<script type="math/tex">Q</script>即可。利用数学上的转换，可以使得<script type="math/tex">\beta_k</script>的计算只需用到当前迭代点的函数值和梯度值。</p>
<p>主要包含三种修正方式：</p>
<ul>
<li><p>Hestenes-Stiefel公式： $$\beta_k=\frac{{\boldsymbol{g}^{(k+1)}}^T[\boldsymbol{g}^{(k+1)}-\boldsymbol{g}^{(k)}]}{{\boldsymbol{d}^{(k)}}^T[\boldsymbol{g}^{(k+1)}-\boldsymbol{g}^{(k)}]}$$</p>
</li>
<li><p>Polak-Ribiere公式：$$\beta_k=\frac{{\boldsymbol{g}^{(k+1)}}^T[\boldsymbol{g}^{(k+1)}-\boldsymbol{g}^{(k)}]}{{\boldsymbol{g}^{(k)}}^T\boldsymbol{g}^{(k)}}$$</p>
</li>
<li><p>Fletcher-Reeves公式：      $$\beta_k=\frac{{\boldsymbol{g}^{(k+1)}}^T\boldsymbol{g}^{(k+1)}}{{\boldsymbol{g}^{(k)}}^T\boldsymbol{g}^{(k)}}$$</p>
</li>
</ul>
<p>对于二次型问题，这三个公式是等价的。但是当目标函数为一般的非线性函数时，这三者并不一致。</p>
<p>对于非二次型问题，共轭梯度法通常不会在<script type="math/tex">n</script>步之内收敛到极小点，随着迭代的进行，搜索方向将不再是<script type="math/tex">Q</script>共轭方向。因此一般会在每经过几次迭代后都重新将搜索方向初始化为目标函数梯度的负方向，然后继续搜索直到满足停止准则。并且对于非二次型问题，精确的一维搜索非常重要。</p>
<h4 id="7-1-4-拟牛顿法"><a href="#7-1-4-拟牛顿法" class="headerlink" title="7.1.4 拟牛顿法"></a>7.1.4 拟牛顿法</h4><h5 id="7-1-4-1-拟牛顿法引言"><a href="#7-1-4-1-拟牛顿法引言" class="headerlink" title="7.1.4.1 拟牛顿法引言"></a>7.1.4.1 拟牛顿法引言</h5><p>在牛顿法中，每一次迭代时的搜索方向为<script type="math/tex">-\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}\boldsymbol{g}^{(k)}</script>，为了避免求Hessian矩阵的逆或者求解<script type="math/tex">n</script>维线性方程组，可以通过设计<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}</script>的近似矩阵<script type="math/tex">H_k</script>来代替<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}</script>，这就是拟牛顿法的基本思路。<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}</script>的近似矩阵<script type="math/tex">H_k</script>随着迭代不断更新，使其拥有<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}</script>的部分性质。</p>
<p>为了使每一次迭代时目标函数值是下降的，即迭代方向<script type="math/tex">-H_k\boldsymbol{g}^{(k)}</script>是下降方向，需要使近似矩阵<script type="math/tex">H_k</script>是正定的。</p>
<p>在拟牛顿法中，构造Hessian矩阵逆矩阵的近似矩阵时，只需要用到目标函数值和梯度。</p>
<h5 id="7-1-4-2-Hessian矩阵逆矩阵的近似性质"><a href="#7-1-4-2-Hessian矩阵逆矩阵的近似性质" class="headerlink" title="7.1.4.2 Hessian矩阵逆矩阵的近似性质"></a>7.1.4.2 Hessian矩阵逆矩阵的近似性质</h5><p>我们用<script type="math/tex">H_k</script>表示Hessian矩阵逆矩阵<script type="math/tex">\boldsymbol{F}(\boldsymbol{x}^{(k)})^{-1}</script>的近似矩阵，在每一次迭代时，我们都会得到一个近似矩阵，因此我们实际上要求解一系列的近似矩阵<script type="math/tex">H_0,H_1,H_2,...</script>。首先我们要讨论这些近似矩阵应该满足的条件，这是拟牛顿法的基础。</p>
<p>当目标为二次型函数时，近似矩阵必须满足： $$H_{k+1}\Delta \boldsymbol{g}^{(i)}=\Delta \boldsymbol{x}^{(i)},0\le i\le k$$</p>
<p>其中，<script type="math/tex">\Delta \boldsymbol{g}^{(i)}=\boldsymbol{g}^{(i+1)}- \boldsymbol{g}^{(i)},\Delta \boldsymbol{x}^{(i)}=\boldsymbol{x}^{(i+1)}- \boldsymbol{x}^{(i)}</script>。</p>
<h5 id="7-1-4-3-具体算法"><a href="#7-1-4-3-具体算法" class="headerlink" title="7.1.4.3 具体算法"></a>7.1.4.3 具体算法</h5><p>下图是基于秩1修正公式的拟牛顿法迭代框架：</p>
<p><img src="/img/NLP/拟牛顿法模板.jpg" srcset="/img/loading.gif" lazyload alt="拟牛顿法模板" style="zoom:15%;" /></p>
<p>所有的拟牛顿法的迭代框架都是类似的，区别仅在于从<script type="math/tex">H_k</script>到<script type="math/tex">H_{k+1}</script>的更新公式不同。</p>
<p>在这里主要讨论三种更新方式：</p>
<ul>
<li><p><strong>秩1修正公式：</strong> $$H_{k+1}=H_k+\frac{(\Delta \boldsymbol{x}^{(k)}-H_k\boldsymbol{g}^{(k)})(\Delta \boldsymbol{x}^{(k)}-H_k\boldsymbol{g}^{(k)})^T}{{\Delta \boldsymbol{g}^{(k)}}^T(\Delta \boldsymbol{x}^{(k)}-H_k\boldsymbol{g}^{(k)})}$$</p>
<p>秩1算法的主要缺点有两个：</p>
<ul>
<li>该算法产生的矩阵<script type="math/tex">H_{k+1}</script>不一定是正定的。</li>
</ul>
</li>
<li><p>如果<script type="math/tex">{\Delta \boldsymbol{g}^{(k)}}^T(\Delta \boldsymbol{x}^{(k)}-H_k\boldsymbol{g}^{(k)})</script>接近于0，计算<script type="math/tex">H_{k+1}</script>可能会有一些困难。</p>
</li>
<li><p><strong>DFP算法：</strong>$$H_{k+1}=H_k+\frac{\Delta \boldsymbol{x}^{(k)}{\Delta \boldsymbol{x}^{(k)}}^T}{{\Delta \boldsymbol{x}^{(k)}}^T\Delta \boldsymbol{g}^{(k)}}-\frac{[H_k\Delta \boldsymbol{g}^{(k)}]{[H_k\Delta \boldsymbol{g}^{(k)}]}^T}{{\Delta \boldsymbol{g}^{(k)}}^TH_k\Delta \boldsymbol{g}^{(k)}}$$</p>
<p>DPF算法可以使得矩阵<script type="math/tex">H_{k+1}</script>保持正定。但是，当处理一些规模较大的非二次型问题时，DFP有时会被卡住，迭代无法开展，主要是因为矩阵<script type="math/tex">H_k</script>接近成为奇异矩阵。</p>
</li>
<li><p><strong>BFGS算法：</strong>   $$H_{k+1}=H_k+(1-\frac{{\Delta \boldsymbol{g}^{(k)}}^TH_k{\Delta \boldsymbol{g}^{(k)}}}{{\Delta \boldsymbol{g}^{(k)}}^T\Delta \boldsymbol{x}^{(k)}})\frac{\Delta \boldsymbol{x}^{(k)}{\Delta \boldsymbol{x}^{(k)}}^T}{{\Delta \boldsymbol{x}^{(k)}}^T\Delta \boldsymbol{g}^{(k)}}-\frac{H_k\Delta \boldsymbol{g}^{(k)}{\Delta \boldsymbol{x}^{(k)}}^T+{(H_k\Delta \boldsymbol{g}^{(k)}{\Delta \boldsymbol{x}^{(k)}}^T)}^T}{{\Delta \boldsymbol{g}^{(k)}}^T\Delta \boldsymbol{x}^{(k)}}$$</p>
<p>BFGS算法能够使得近似矩阵<script type="math/tex">H_k</script>一直保持正定。并且在迭代过程中一维搜索的精度不高时，BFGS算法仍然比较稳健。                                            </p>
</li>
</ul>
<h4 id="7-1-5-信赖域法"><a href="#7-1-5-信赖域法" class="headerlink" title="7.1.5 信赖域法"></a>7.1.5 信赖域法</h4><p>这里直接给出《最优化：建模、算法与理论》中关于信赖域法的描述：</p>
<p>信赖域法的框架为：</p>
<center class="half">
    <img src="/img/NLP/信赖域法框架1.jpg" srcset="/img/loading.gif" lazyload width="200"/>
    <img src="/img/NLP/信赖域法框架2.jpg" srcset="/img/loading.gif" lazyload width="225"/>
</center>


<p>具体算法为：</p>
<p><img src="/img/NLP/信赖域法算法.jpg" srcset="/img/loading.gif" lazyload style="zoom:30%;" /></p>
<h3 id="7-2-约束优化"><a href="#7-2-约束优化" class="headerlink" title="7.2 约束优化"></a>7.2 约束优化</h3><p>约束优化的内容非常丰富，不同的问题往往有不同的特点，我们需要根据每一种问题的具体特点设计最适合该问题的算法。约束优化中，没有万能算法，只要是合适问题的算法就是好算法，我们不能用算法去套问题，而是要结合每一个问题的特色找到最适合该问题的算法。常用的约束优化问题及算法如下图所示：</p>
<p><img src="/img/NLP/约束优化概览.jpg" srcset="/img/loading.gif" lazyload alt="约束优化概览" style="zoom:23%;" /></p>
<p>接下来简要介绍几种常用的约束优化算法。</p>
<h4 id="7-2-1-投影方向法"><a href="#7-2-1-投影方向法" class="headerlink" title="7.2.1 投影方向法"></a>7.2.1 投影方向法</h4><p>投影方向法的思想很简单，就是将无约束优化问题中计算出的新的迭代点投影到约束集中，让每一次产生的迭代点都满足约束条件。</p>
<p>无约束优化问题的迭代公式为： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}$$</p>
<p>但是新产生的迭代点<script type="math/tex">\boldsymbol{x}^{(k+1)}</script>可能不满足问题的约束条件，即点<script type="math/tex">\boldsymbol{x}^{(k+1)}</script>可能不在约束集<script type="math/tex">\Omega</script>中。因此，一种简单的改进方式就是引入投影。如果<script type="math/tex">\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}</script>在<script type="math/tex">\Omega</script>内，那么就令新的迭代点<script type="math/tex">\boldsymbol{x}^{(k+1)}</script>等于<script type="math/tex">\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}</script>；否则，如果<script type="math/tex">\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}</script>不在<script type="math/tex">\Omega</script>内，应该将其投影到<script type="math/tex">\Omega</script>中，并令投影结果作为新的迭代点<script type="math/tex">\boldsymbol{x}^{(k+1)}</script>。</p>
<p>如果记<script type="math/tex">\boldsymbol{\Pi}</script>为投影算子，称点<script type="math/tex">\boldsymbol{\Pi}[\boldsymbol{x}]</script>为点<script type="math/tex">\boldsymbol{x}</script>在<script type="math/tex">\Omega</script>上的投影。</p>
<p>利用投影算子，则改进后的迭代过程为：$$\boldsymbol{x}^{(k+1)}=\boldsymbol{\Pi}[\boldsymbol{x}^{(k)}+\alpha_k\boldsymbol{d}^{(k)}]$$</p>
<p>对于闭凸集而言，投影算子有如下的定义： $$\boldsymbol{\Pi}[\boldsymbol{x}]=\underset{\boldsymbol{z}\in\Omega}{argmin}\ \ {||\boldsymbol{z}-\boldsymbol{x}||}$$</p>
<p>意味着<script type="math/tex">\boldsymbol{\Pi}[\boldsymbol{x}]</script>为<script type="math/tex">\Omega</script>中”最接近”<script type="math/tex">\boldsymbol{x}</script>的点。</p>
<h4 id="7-2-2-拉格朗日法"><a href="#7-2-2-拉格朗日法" class="headerlink" title="7.2.2 拉格朗日法"></a>7.2.2 拉格朗日法</h4><p>基于拉格朗日函数的求解方法的基本思路是利用梯度法在更新决策变量的同时更新拉格朗日乘子向量。</p>
<ul>
<li><p>针对<strong>仅含等式约束</strong>优化问题的拉格朗日法： $$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_k(\nabla f(\boldsymbol{x}^{(k)})+D\boldsymbol{h}(\boldsymbol{x}^{(k)})^T{\boldsymbol{\lambda}^{(k)}})$$  $$\boldsymbol{\lambda}^{(k+1)}=\boldsymbol{\lambda}^{(k)}+\beta_k\boldsymbol{h}(\boldsymbol{x}^{(k)})$$</p>
<p>可以看出，<script type="math/tex">\boldsymbol{x}^{(k)}</script>的更新方程是一种使得拉格朗日函数关于自变量<script type="math/tex">\boldsymbol{x}</script>极小化的梯度算法；<script type="math/tex">\boldsymbol{\lambda}^{(k)}</script>的更新方程也是一种梯度方程，使得拉格朗日函数关于自变量<script type="math/tex">\boldsymbol{\lambda}</script>极大化。</p>
</li>
<li><p>针对<strong>含不等式约束</strong>优化问题的拉格朗日法：$$\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_k(\nabla f(\boldsymbol{x}^{(k)})+D\boldsymbol{g}(\boldsymbol{x}^{(k)})^T{\boldsymbol{\mu}^{(k)}})$$   $$\boldsymbol{\mu}^{(k+1)}=\max\{\boldsymbol{\mu}^{(k)}+\beta_k\boldsymbol{g}(\boldsymbol{x}^{(k)}),0\}$$</p>
<p>和上面一样，<script type="math/tex">\boldsymbol{x}^{(k)}</script>的更新方程是一种梯度法，使得拉格朗日函数关于自变量<script type="math/tex">\boldsymbol{x}</script>极小化；<script type="math/tex">\boldsymbol{\mu}^{(k)}</script>的更新方程是一种投影梯度法，使得拉格朗日函数关于自变量<script type="math/tex">\boldsymbol{\mu}</script>极大化。</p>
</li>
</ul>
<h4 id="7-2-3-罚函数法"><a href="#7-2-3-罚函数法" class="headerlink" title="7.2.3 罚函数法"></a>7.2.3 罚函数法</h4><p>罚函数法通过引入罚函数的概念，将有约束优化问题转化为无约束优化问题进行求解。</p>
<p>即将原始的约束优化问题： $$\min  \ \ \ \ \ f(\boldsymbol{x})$$   $$ s.t. \ \ \ \ \boldsymbol{x}\in \Omega$$</p>
<p>转化为下述无约束优化问题：   $$\min  \ \ \ \ \ f(\boldsymbol{x})+\gamma P(\boldsymbol{x})$$</p>
<p>其中<script type="math/tex">P(\boldsymbol{x})</script>称为罚函数。罚函数的主要作用在于对可行集之外的点进行惩罚。</p>
<h2 id="8-初始点的选取"><a href="#8-初始点的选取" class="headerlink" title="8 初始点的选取"></a><strong>8 初始点的选取</strong></h2><p>在非线性规划中，如何选取一个好的初始点更像是一个工程问题而非数学问题。在非凸优化问题中，迭代算法很有可能陷入一个局部最优解而无法找到全局最优解。因此，工程上一个常见的做法是，随机在全局选取很多个点作为初始点开始优化算法的迭代过程，如果有很多个初始点最终收敛到同一个点，那么这个点很有可能就是全局最小点。或者也可以在所有的最终收敛点中选择目标函数值最小的一个。在一篇关于轨迹优化的入门论文中，通常是从一个满足动力学约束的显而易见的轨迹开始迭代，最终得到一个所需动力最小的最优轨迹。</p>
<h2 id="9-终止准则"><a href="#9-终止准则" class="headerlink" title="9 终止准则"></a><strong>9 终止准则</strong></h2><p>一般而言，可以利用局部极小点的一阶必要条件作为终止准则，比如说无约束优化和集合优化问题可以用梯度为0作为迭代的终止条件，等式优化问题可以使用拉格朗日条件作为终止条件，而含有不等式约束的优化问题可以使用KKT条件。</p>
<p>但是在实际应用中，采用数值计算方法很难恰好得到梯度为0的结果。所以一种很实用的停止准则是采用梯度的范数<script type="math/tex">||\nabla f(\boldsymbol{x}^{(k)})||</script>，如果梯度的范数小于某个预设的阈值，则迭代停止。此外，还可以计算两个相邻迭代点对应的目标函数值之差的绝对值<script type="math/tex">|f(\boldsymbol{x}^{(k+1)})-f(\boldsymbol{x}^{(k)})|</script>，如果小于某个阈值，则迭代停止。还有一种停止规则是计算相邻两个迭代点差值的范数<script type="math/tex">||\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}||</script>，如果小于某个阈值，则迭代停止。</p>
<p>针对无约束优化和集合优化问题，总结如下：</p>
<ul>
<li>绝对的停止准则一般有：<ul>
<li>梯度的范数：<script type="math/tex">||\nabla f(\boldsymbol{x}^{(k)})||<\varepsilon</script></li>
<li>相邻迭代点对应的目标函数值之差的绝对值：<script type="math/tex">|f(\boldsymbol{x}^{(k+1)})-f(\boldsymbol{x}^{(k)})|<\varepsilon</script></li>
<li>相邻两个迭代点差值的范数：<script type="math/tex">||\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}||<\varepsilon</script></li>
</ul>
</li>
<li>上述停止规则还可以改为相对值：<ul>
<li>相对的相邻迭代点对应的目标函数值之差的绝对值：<script type="math/tex">\frac{|f(\boldsymbol{x}^{(k+1)})-f(\boldsymbol{x}^{(k)})|}{|f(\boldsymbol{x}^{(k)})|}<\varepsilon</script></li>
<li>相对的相邻两个迭代点差值的范数：<script type="math/tex">\frac{||\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}||}{||\boldsymbol{x}^{(k)}||}<\varepsilon</script></li>
</ul>
</li>
<li>为了避免相对停止规则的分母过小，可以做如下修改：<ul>
<li>目标函数之差：<script type="math/tex">\frac{|f(\boldsymbol{x}^{(k+1)})-f(\boldsymbol{x}^{(k)})|}{max\{1,|f(\boldsymbol{x}^{(k)})|\}}<\varepsilon</script></li>
<li>迭代点差值的范数：<script type="math/tex">\frac{||\boldsymbol{x}^{(k+1)}-\boldsymbol{x}^{(k)}||}{max\{1,||\boldsymbol{x}^{(k)}||\}}<\varepsilon</script></li>
</ul>
</li>
</ul>
<h2 id="10-参考资料"><a href="#10-参考资料" class="headerlink" title="10 参考资料"></a>10 参考资料</h2><p>[1] 《最优化导论》 (非常好的一本入门书)</p>
<p>[2]  西安交通大学运筹学课程翟桥柱老师的课件 (老师的课件简直太好了！)</p>
<p>[3] 《最优化：建模、算法与理论》关于最优化理论更为全面、深入的介绍</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Math/">Math</a>
                    
                      <a class="hover-with-bg" href="/tags/Optimization/">Optimization</a>
                    
                      <a class="hover-with-bg" href="/tags/Nonlinear-Programming/">Nonlinear Programming</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/12/04/%E5%9B%9B%E6%97%8B%E7%BF%BC%E9%A3%9E%E6%8E%A7%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/">
                        <span class="hidden-mobile">四旋翼飞控整体框架介绍：建模、状态估计和控制</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  










  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
